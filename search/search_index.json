{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"\ud83d\udcd8 Introduction to Data Science \u2014 Textbook","text":"<p>A compact, classroom-ready textbook with theory, by-hand examples.</p> <p> </p>"},{"location":"index.html#what-youll-learn","title":"\ud83c\udfaf What you\u2019ll learn","text":"<ul> <li>Solid foundations of Data Science: concepts, math, and workflow</li> <li>Visualization: histograms, scatter, KDE, Q\u2013Q, boxplots, multivariate charts</li> <li>Modeling: regression, classification, clustering, evaluation, cross-validation</li> <li>Data preparation: cleaning, integration, reduction, transformation, discretization</li> <li>Hands-on: Python code to generate figures used in the chapters</li> </ul>"},{"location":"index.html#data-science-overview","title":"Data Science Overview","text":"<pre><code>graph TD\n    A[Data Science] --&gt; B[Prior Knowledge]\n    A --&gt; C[Data Preparation]\n    A --&gt; D[Modeling]\n    A --&gt; E[Evaluation]\n    A --&gt; F[Deployment]\n    C --&gt; C1[Cleaning]\n    C --&gt; C2[Integration]\n    C --&gt; C3[Reduction]\n    D --&gt; D1[Regression]\n    D --&gt; D2[Classification]\n    D --&gt; D3[Clustering]</code></pre>"},{"location":"index.html#how-to-use-this-site","title":"\ud83d\uddc2\ufe0f How to use this site","text":"<ul> <li>Use the left navigation to jump between chapters.</li> <li>Each chapter includes formulas (MathJax), diagrams (Mermaid/PNG), and code snippets to reproduce figures.</li> <li>Downloadable <code>.md</code> and <code>.png</code> are available where relevant for GitHub Pages hosting.</li> </ul> <p>\ud83d\udca1 Tip: Use the search box (\u2318/Ctrl + K) to find formulas, terms, or figure names instantly.</p>"},{"location":"index.html#chapters","title":"\ud83d\udcda Chapters","text":"<ul> <li>Foundations</li> <li>Chapter 1 \u2014 Introduction </li> <li>Chapter 2 \u2014 What is Data Science </li> <li>Chapter 3 \u2014 Data Objects and Attribute Types </li> <li> <p>Chapter 4 \u2014 Basic Statistical Descriptions of Data</p> </li> <li> <p>Visualization</p> </li> <li> <p>Chapter 5 \u00b7 Chapter 6 \u00b7 Chapter 7 \u00b7 Chapter 8</p> </li> <li> <p>Process &amp; Preparation</p> </li> <li> <p>Chapter 9 \u00b7 Chapter 10 \u00b7 Chapter 11 \u00b7 Chapter 12 \u00b7 Chapter 13</p> </li> <li> <p>Modeling &amp; Evaluation</p> </li> <li>Chapter 14 \u00b7 Chapter 15 \u00b7 Chapter 16 \u00b7 Chapter 17</li> </ul>"},{"location":"index.html#plot-gallery-quick-preview","title":"\ud83d\uddbc\ufe0f Plot Gallery (quick preview)","text":"Quantile Histogram Scatter Multi-Scatter Scatter Matrix Bubble KDE Q\u2013Q Parallel Coordinates Deviation Andrews Curves Box"},{"location":"Chapter1.html","title":"\ud83d\udcd8 Chapter 1 \u2014 Introduction","text":"<p>Understanding the relationship among Artificial Intelligence (AI), Machine Learning (ML), and Data Science (DS) \u2014 their evolution, importance, and real-world impact.</p>"},{"location":"Chapter1.html#1-relation-among-ai-ml-and-data-science","title":"1. Relation among AI, ML, and Data Science","text":""},{"location":"Chapter1.html#conceptual-overview","title":"Conceptual Overview","text":"<p>Artificial Intelligence (AI) is the broadest field, aiming to create systems capable of intelligent behavior.  </p> <p>Machine Learning (ML) is a subset of AI that focuses on algorithms that learn patterns from data. Data Science (DS) integrates statistics, data analysis, and ML to extract insights and build predictive models.</p> <p>The hierarchical relationship can be expressed as:</p> \\[ \\text{Data Science} \\subseteq (\\text{Machine Learning}) \\subseteq (\\text{Artificial Intelligence}) \\] <p>This relationship is visualized as follows:</p> <p></p>"},{"location":"Chapter1.html#mathematical-view","title":"Mathematical View","text":"<p>Data Science involves modeling real-world processes mathematically:</p> \\[ Y = f(X) + \\varepsilon \\] <p>Where: - \\(X\\) = Input features or observed data - \\(Y\\) = Target variable or output - \\(f(X)\\) = Learned function (model) - \\(\\varepsilon\\) = Random noise or error term</p> <p>Example:  Predicting house prices \u2014 \\( Y = f(\\text{area, rooms, location}) + \\varepsilon \\)</p>"},{"location":"Chapter1.html#2-evolution-of-data-science","title":"2. Evolution of Data Science","text":"<p>The discipline evolved through several key phases, blending statistics, computing, and AI:</p> Era Focus Key Development 1950s Artificial Intelligence Symbolic reasoning, rule-based systems 1980s Machine Learning Statistical learning, neural networks 2010s Deep Learning GPU-driven deep neural networks 2020s Data Science Unified approach: data + ML + domain knowledge <p>Timeline Visualization:</p> <p></p>"},{"location":"Chapter1.html#3-importance-of-data-science","title":"3. Importance of Data Science","text":"<p>Data Science lies at the intersection of mathematics, programming, and domain expertise. It powers modern decision-making, enabling organizations to:</p> <ul> <li>Detect patterns in large-scale data  </li> <li>Automate predictions and recommendations  </li> <li>Optimize resources and operations  </li> <li>Support data-driven policies and research</li> </ul>"},{"location":"Chapter1.html#data-science-workflow","title":"Data Science Workflow","text":"<p>Key Steps: 1. Data Collection \u2014 Gather raw structured and unstructured data 2. Data Preparation \u2014 Handle missing values, noise, and inconsistencies 3. Exploratory Analysis \u2014 Visualize and understand distributions 4. Modeling \u2014 Apply ML/statistical models to learn relationships 5. Evaluation &amp; Deployment \u2014 Assess accuracy and deploy results 6. Iteration \u2014 Feedback loop for model improvement  </p>"},{"location":"Chapter1.html#4-real-world-examples","title":"4. Real-world Examples","text":"Domain Application Description Healthcare Disease Prediction ML models identify risk factors for diseases Finance Fraud Detection Anomaly detection on transaction data Marketing Recommendation Systems Suggest personalized products using user data IoT Predictive Maintenance Sensors predict equipment failures Research Climate Modeling Data analysis for environmental forecasts"},{"location":"Chapter1.html#summary","title":"Summary","text":"<ul> <li>AI aims for intelligent behavior.  </li> <li>ML enables systems to learn from data.  </li> <li>Data Science applies these methods to extract actionable insights.  </li> <li>Its iterative process ensures continuous improvement and generalization.</li> </ul>"},{"location":"Chapter10.html","title":"\ud83e\uddf9 Chapter 10 \u2014 Data Preprocessing","text":"<p>This chapter explains data quality issues and the essential steps in data preprocessing including cleaning, handling missing values, and noise reduction.</p>"},{"location":"Chapter10.html#1-why-preprocess-the-data","title":"\ud83e\udde0 1. Why Preprocess the Data?","text":"<p>Raw data is rarely perfect. It may contain missing values, inconsistencies, noise, or outliers. Data preprocessing ensures the dataset is accurate, complete, and consistent before modeling.</p> <pre><code>graph TD\n    A[Raw Data] --&gt; B[Data Preprocessing]\n    B --&gt; C[Clean Data]\n    C --&gt; D[Model Training]</code></pre>"},{"location":"Chapter10.html#key-reasons","title":"Key Reasons","text":"<ol> <li>Improved Model Accuracy \u2014 Clean data leads to better predictions.</li> <li>Reduced Complexity \u2014 Simplifies data for analysis.</li> <li>Error Minimization \u2014 Prevents misleading results due to poor data quality.</li> <li>Better Interpretability \u2014 Makes outputs meaningful to domain experts.</li> </ol>"},{"location":"Chapter10.html#2-data-quality-dimensions","title":"\u2699\ufe0f 2. Data Quality Dimensions","text":"Dimension Description Example Issue Accuracy Correctness of data Age recorded as 250 Completeness Missing values or records Null entries in income Consistency Uniform format or scale \u201cUSA\u201d vs \u201cUnited States\u201d Timeliness Up\u2011to\u2011date data Using 2010 sales for 2025 analysis Validity Follows rules and constraints Negative quantity sold Uniqueness Duplicate entries Same customer twice <pre><code>graph LR\n    A[Data Quality] --&gt; B[Accuracy]\n    A --&gt; C[Completeness]\n    A --&gt; D[Consistency]\n    A --&gt; E[Timeliness]\n    A --&gt; F[Validity]\n    A --&gt; G[Uniqueness]</code></pre>"},{"location":"Chapter10.html#3-major-tasks-in-data-preprocessing","title":"\ud83e\udde9 3. Major Tasks in Data Preprocessing","text":"<p>The main stages include:</p> <pre><code>graph TD\n    A[Data Preprocessing] --&gt; B[Data Cleaning]\n    A --&gt; C[Data Integration]\n    A --&gt; D[Data Transformation]\n    A --&gt; E[Data Reduction]\n    A --&gt; F[Data Discretization]</code></pre> Task Description Example Data Cleaning Handle noise, missing, inconsistent data Replace nulls, smooth noise Data Integration Combine data from multiple sources Join customer and sales tables Data Transformation Normalize or encode Min\u2011max scaling, one\u2011hot encoding Data Reduction Reduce volume while maintaining integrity PCA, feature selection Data Discretization Convert continuous to categorical Income ranges: low/medium/high"},{"location":"Chapter10.html#4-data-cleaning","title":"\ud83e\uddfc 4. Data Cleaning","text":""},{"location":"Chapter10.html#41-what-is-data-cleaning","title":"4.1 What is Data Cleaning?","text":"<p>Data Cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset.</p> <pre><code>graph TD\n    A[Raw Data] --&gt; B[Detect Issues]\n    B --&gt; C[Handle Missing Values]\n    C --&gt; D[Handle Noise]\n    D --&gt; E[Remove Duplicates]\n    E --&gt; F[Clean Dataset]</code></pre>"},{"location":"Chapter10.html#5-handling-missing-values","title":"\ud83d\udea7 5. Handling Missing Values","text":""},{"location":"Chapter10.html#51-common-causes","title":"5.1 Common Causes","text":"<ul> <li>Human errors during data entry  </li> <li>Sensor or transmission failures  </li> <li>Non\u2011responses in surveys  </li> </ul>"},{"location":"Chapter10.html#52-strategies","title":"5.2 Strategies","text":"Method Description Example Ignore tuple Remove records with missing values Drop rows with <code>NaN</code> Global constant Fill with a fixed value Replace null with \u201cUnknown\u201d Mean/Median/Mode Use statistical measures Fill income with mean value Predictive Use model\u2011based imputation Regression or k\u2011NN Domain\u2011based Use rules from domain Age = avg for that region"},{"location":"Chapter10.html#manual-example","title":"Manual Example","text":"Record Age Income A 25 60K B NaN 50K C 30 NaN <p>Mean Age = (25+30)/2 = 27.5 Mean Income = (60+50)/2 = 55K</p> Record Age Income A 25 60K B 27.5 50K C 30 55K"},{"location":"Chapter10.html#6-handling-noisy-data","title":"\ud83d\udd0a 6. Handling Noisy Data","text":""},{"location":"Chapter10.html#61-definition","title":"6.1 Definition","text":"<p>Noisy data contains random errors, deviations, or outliers that distort patterns.</p>"},{"location":"Chapter10.html#62-noise-reduction-techniques","title":"6.2 Noise Reduction Techniques","text":"Technique Description Example Binning Sort data and smooth using bins Group ages 20\u201129, 30\u201139 Regression Fit model to remove deviations Linear regression smoothing Clustering Group similar data Remove far\u2011away outliers Moving Average Smooth temporal fluctuations Stock price smoothing <pre><code>graph TD\n    A[Noisy Data] --&gt; B[Binning]\n    A --&gt; C[Regression Smoothing]\n    A --&gt; D[Clustering]\n    A --&gt; E[Moving Average]\n    B &amp; C &amp; D &amp; E --&gt; F[Smoother Dataset]</code></pre>"},{"location":"Chapter10.html#manual-example-binning","title":"Manual Example \u2014 Binning","text":"<p>Data: [5, 6, 7, 10, 12, 13, 20] Sort and divide into 3 bins: - Bin1: [5,6,7] \u2192 mean=6 - Bin2: [10,12,13] \u2192 mean=11.67 - Bin3: [20] \u2192 mean=20 Smoothed data: [6,6,6,11.67,11.67,11.67,20]</p>"},{"location":"Chapter10.html#7-data-cleaning-as-a-process","title":"\ud83e\uddee 7. Data Cleaning as a Process","text":"<p>The cleaning process involves multiple iterative passes:</p> <pre><code>graph TD\n    A[Identify Data Issues] --&gt; B[Select Cleaning Strategy]\n    B --&gt; C[Apply Transformations]\n    C --&gt; D[Verify Results]\n    D --&gt; E[Repeat if Necessary]\n    E --&gt; F[Store Clean Data]</code></pre> Step Action Output Identify Detect missing/outlier values Report summary Strategy Choose imputation or removal Decision plan Apply Execute cleaning rules Updated dataset Verify Compare stats pre/post cleaning Quality metrics Store Save clean data for analysis Ready data file"},{"location":"Chapter10.html#8-summary-diagram","title":"\ud83d\udcc8 8. Summary Diagram","text":"<pre><code>graph TD\n    A[Raw Data] --&gt; B[Detect Issues]\n    B --&gt; C[Handle Missing Values]\n    B --&gt; D[Handle Noisy Data]\n    C --&gt; E[Verify and Validate]\n    D --&gt; E\n    E --&gt; F[Clean Dataset]\n    F --&gt; G[Integration &amp; Transformation]</code></pre>"},{"location":"Chapter10.html#9-key-takeaways","title":"\ud83e\udde0 9. Key Takeaways","text":"<ul> <li>Data preprocessing is vital for reliable modeling.  </li> <li>Major steps include cleaning, integration, transformation, reduction, and discretization.  </li> <li>Handling missing and noisy data is critical for data quality.  </li> <li>Cleaning is an iterative and domain\u2011driven process.  </li> </ul>"},{"location":"Chapter10.html#10-exam-practice-questions","title":"\ud83d\udcd8 10. Exam Practice Questions","text":"<ol> <li>What are the main reasons for preprocessing data?</li> <li>Describe at least three techniques for handling missing values. </li> <li>Explain the concept of noisy data with examples. </li> <li>Outline the stages of the data cleaning process.</li> <li>Why is iterative cleaning important in real\u2011world data science projects?</li> </ol>"},{"location":"Chapter11.html","title":"\ud83e\udde9 Chapter 11 \u2014 Data Integration","text":"<p>This chapter explains Data Integration, focusing on entity identification, redundancy and correlation analysis, tuple duplication, and data value conflict resolution.</p>"},{"location":"Chapter11.html#1-what-is-data-integration","title":"\ud83d\udd17 1. What is Data Integration?","text":"<p>Data Integration is the process of combining data from multiple sources into a unified and consistent view. It\u2019s crucial for analytical systems that rely on data from heterogeneous sources (e.g., databases, files, APIs).</p> <pre><code>graph TD\n    A[Source 1] --&gt; E[Integrated Data]\n    B[Source 2] --&gt; E\n    C[Source 3] --&gt; E</code></pre>"},{"location":"Chapter11.html#example","title":"Example","text":"<ul> <li>Integrating customer data from CRM, ERP, and web analytics systems.  </li> <li>Ensures consistent customer IDs, names, and contact details across all sources.</li> </ul>"},{"location":"Chapter11.html#2-the-entity-identification-problem","title":"\ud83e\udde0 2. The Entity Identification Problem","text":""},{"location":"Chapter11.html#definition","title":"Definition","text":"<p>The Entity Identification Problem (also called Record Linkage or Entity Resolution) occurs when the same real-world entity appears under different names or identifiers across datasets.</p> Example System A System B Customer Name \u201cJohn Smith\u201d \u201cJ. A. Smith\u201d Customer ID 1452 CUST\u2011001452 Address 12 Oak Rd, NY 12 Oak Road, New York <p>The challenge is to determine whether these records refer to the same entity.</p> <pre><code>graph LR\n    A[Record in Source 1] --&gt; C[Entity Matching]\n    B[Record in Source 2] --&gt; C\n    C --&gt; D[Unified Entity Record]</code></pre>"},{"location":"Chapter11.html#solutions","title":"Solutions","text":"<ol> <li>Rule\u2011based matching \u2014 exact or fuzzy string matching.  </li> <li>Machine Learning approaches \u2014 classify record pairs as \u201csame/different.\u201d  </li> <li>Unique key assignment \u2014 generate global identifiers.  </li> </ol>"},{"location":"Chapter11.html#3-redundancy-and-correlation-analysis","title":"\ud83d\udd01 3. Redundancy and Correlation Analysis","text":""},{"location":"Chapter11.html#31-redundancy","title":"3.1 Redundancy","text":"<p>Redundancy refers to duplicate or repetitive data that doesn\u2019t add new information. It wastes storage and may bias statistical analysis.</p> Example Redundant Data Customer table Repeated address field Joined tables Duplicate keys after merge <pre><code>graph TD\n    A[Data Source 1] --&gt; B[Integration Process]\n    C[Data Source 2] --&gt; B\n    B --&gt; D[Redundant Attributes Found]\n    D --&gt; E[Remove or Merge Redundant Data]</code></pre>"},{"location":"Chapter11.html#32-correlation-analysis","title":"3.2 Correlation Analysis","text":"<p>Correlation analysis helps identify relationships between variables to remove redundant attributes.</p> <p>Mathematically, for attributes \\(X\\) and \\(Y\\):</p> \\[ r_{XY} = \\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})} {\\sqrt{\\sum_i (x_i - \\bar{x})^2 \\sum_i (y_i - \\bar{y})^2}} \\] <p>If \\(|r_{XY}|\\) \u2248 1 \u2192 strong correlation \u2192 one attribute may be redundant.</p> X Y (X\u2013mean) (Y\u2013mean) Product 1 2 -1 -2 2 2 3 0 -1 0 3 4 +1 0 0 Mean 2 3 \u2014 \u2014 $$ r_{XY} $$ \u2248 1"},{"location":"Chapter11.html#4-tuple-duplication","title":"\ud83e\uddec 4. Tuple Duplication","text":""},{"location":"Chapter11.html#definition_1","title":"Definition","text":"<p>Tuple duplication occurs when the same data record appears multiple times in a dataset, either within or across sources.</p> <pre><code>graph TD\n    A[Source 1] --&gt; D[Duplicate Detector]\n    B[Source 2] --&gt; D\n    D --&gt; E[Remove or Merge Duplicates]</code></pre>"},{"location":"Chapter11.html#causes","title":"Causes","text":"<ul> <li>Repeated data imports  </li> <li>Manual data entry  </li> <li>Join operations without distinct filters  </li> </ul>"},{"location":"Chapter11.html#detection-removal","title":"Detection &amp; Removal","text":"Method Description Exact matching Compare full tuples Key-based Check duplicate primary keys Hash-based Use hash functions for large datasets Clustering Group similar tuples"},{"location":"Chapter11.html#example_1","title":"Example","text":"ID Name City 1 John NY 2 Alice TX 1 John NY <p>\u2192 Duplicate tuple (ID=1).  </p> <p>SQL Example: SQL<pre><code>SELECT Name, City, COUNT(*) \nFROM Customers\nGROUP BY Name, City\nHAVING COUNT(*) &gt; 1;\n</code></pre></p>"},{"location":"Chapter11.html#5-data-value-conflict-detection-and-resolution","title":"\u2696\ufe0f 5. Data Value Conflict Detection and Resolution","text":""},{"location":"Chapter11.html#definition_2","title":"Definition","text":"<p>Occurs when the same entity has different attribute values in different data sources.</p> Source Customer City CRM John Doe \u201cNew York\u201d ERP John Doe \u201cNYC\u201d"},{"location":"Chapter11.html#detection","title":"Detection","text":"<ul> <li>Compare same\u2011entity attributes across sources.  </li> <li>Use string similarity (e.g., Levenshtein distance) or standardized dictionaries.</li> </ul>"},{"location":"Chapter11.html#resolution-strategies","title":"Resolution Strategies","text":"Strategy Description Example Rule-based Apply business rules Prefer ERP data for \u201cCity\u201d Confidence score Assign trust levels to sources Weight by reliability Most recent Use timestamp to pick latest Choose data from latest update Merge Combine differing values City = \u201cNew York (NYC)\u201d <pre><code>graph TD\n    A[Conflicting Values] --&gt; B[Detection Algorithm]\n    B --&gt; C[Apply Rules / Scoring]\n    C --&gt; D[Resolved Value]</code></pre>"},{"location":"Chapter11.html#6-example-integration-scenario","title":"\ud83e\udde9 6. Example Integration Scenario","text":"<pre><code>graph LR\n    A[CRM System] --&gt; D[Entity Matching]\n    B[ERP System] --&gt; D\n    C[Web Logs] --&gt; D\n    D --&gt; E[Unified Customer View]\n    E --&gt; F[Clean and Consistent Data Warehouse]</code></pre>"},{"location":"Chapter11.html#stepwise-illustration","title":"Step\u2011wise Illustration","text":"Step Operation Result 1 Match \u201cJohn Smith\u201d across systems Entity identified 2 Remove duplicates Unique records 3 Resolve city conflicts \u201cNew York\u201d unified 4 Remove redundant fields Reduced schema 5 Store unified customer data Ready for analytics"},{"location":"Chapter11.html#7-summary-table","title":"\ud83e\udde0 7. Summary Table","text":"Task Description Techniques Entity Identification Match entities across sources String similarity, ML classification Redundancy Analysis Detect duplicated data or attributes Correlation, schema mapping Tuple Duplication Detect repeated records Hashing, grouping, clustering Conflict Resolution Resolve conflicting values Rule-based, timestamp, confidence weight"},{"location":"Chapter11.html#8-practice-questions","title":"\ud83d\udcd8 8. Practice Questions","text":"<ol> <li>Define the Entity Identification Problem and explain its challenges. </li> <li>How does correlation analysis help in reducing redundancy? </li> <li>Explain tuple duplication with SQL and manual examples. </li> <li>What are common strategies for data conflict resolution? </li> <li>Why is integration critical in enterprise data warehouses? </li> </ol>"},{"location":"Chapter12.html","title":"\ud83d\udcc9 Chapter 12 \u2014 Data Reduction","text":"<p>This chapter explores the need for Data Reduction, its strategies, and key techniques like Wavelet Transformation, Principal Component Analysis (PCA), and Attribute Subset Selection.</p>"},{"location":"Chapter12.html#1-overview-of-data-reduction","title":"\ud83d\udd01 1. Overview of Data Reduction","text":"<p>Data Reduction aims to obtain a reduced representation of a dataset that is smaller in volume but produces almost the same analytical results.</p> <pre><code>graph TD\n    A[Raw Large Dataset] --&gt; B[Reduction Techniques]\n    B --&gt; C[Reduced Dataset]\n    C --&gt; D[Efficient Analysis]</code></pre>"},{"location":"Chapter12.html#why-data-reduction","title":"Why Data Reduction?","text":"<ul> <li>To reduce storage and computation costs</li> <li>To improve algorithm efficiency</li> <li>To eliminate redundant or irrelevant features</li> <li>To enhance interpretability without losing essential information</li> </ul>"},{"location":"Chapter12.html#2-strategies-for-data-reduction","title":"\ud83e\udde9 2. Strategies for Data Reduction","text":"Strategy Description Example Techniques Data Cube Aggregation Summarize data at multiple granularities Roll-up operations in OLAP Dimensionality Reduction Reduce number of attributes PCA, Feature selection Numerosity Reduction Replace data with smaller models Clustering, Regression models Data Compression Encode data efficiently Wavelet transform, Huffman coding <pre><code>graph TD\n    A[Data Reduction] --&gt; B[Dimensionality Reduction]\n    A --&gt; C[Numerosity Reduction]\n    A --&gt; D[Data Compression]</code></pre>"},{"location":"Chapter12.html#3-wavelet-transformation","title":"\ud83c\udf0a 3. Wavelet Transformation","text":""},{"location":"Chapter12.html#definition","title":"Definition","text":"<p>The Wavelet Transform decomposes data into components of different frequency sub-bands \u2014 offering both time and frequency information.</p> <p>Unlike the Fourier Transform, which uses only sine and cosine waves, Wavelets use localized basis functions.</p>"},{"location":"Chapter12.html#31-mathematical-concept","title":"3.1 Mathematical Concept","text":"<p>A signal \\( f(t) \\) can be represented as: \\(f(t) = \\sum_{j,k} c_{j,k} \\, \\psi_{j,k}(t)\\)</p> <p>where - \\( \\psi_{j,k}(t) \\) are wavelet basis functions at scale j and translation k - \\( c_{j,k} \\) are wavelet coefficients.</p>"},{"location":"Chapter12.html#32-example-haar-wavelet-transformation","title":"3.2 Example \u2014 Haar Wavelet Transformation","text":"<p>Original Data: [4, 6, 10, 12]</p> <ol> <li>Pairwise averages and differences:  </li> <li>Level 1 Averages: [(4+6)/2, (10+12)/2] = [5, 11]  </li> <li> <p>Level 1 Differences: [(4\u22126)/2, (10\u221212)/2] = [\u22121, \u22121]</p> </li> <li> <p>Repeat on averages:  </p> </li> <li>Level 2 Average: (5+11)/2 = 8  </li> <li>Level 2 Difference: (5\u221211)/2 = \u22123</li> </ol> <p>Wavelet Coefficients: [8, \u22123, \u22121, \u22121]</p> <pre><code>graph TD\n    A[Raw Data: 4,6,10,12] --&gt; B[Compute Averages &amp; Differences]\n    B --&gt; C[Level 1: #40; 5,11 #41;, #40; -1,-1 #41;]\n    C --&gt; D[Level 2: #40; 8,-3 #41;]\n    D --&gt; E[Coefficients: 8, -3, -1, -1]</code></pre> <p>Wavelet coefficients provide a compressed representation while retaining key trends.</p>"},{"location":"Chapter12.html#4-principal-component-analysis-pca","title":"\ud83e\uddee 4. Principal Component Analysis (PCA)","text":""},{"location":"Chapter12.html#41-purpose","title":"4.1 Purpose","text":"<p>PCA reduces dimensionality by transforming correlated features into a new set of uncorrelated variables (principal components) that capture maximum variance.</p> <pre><code>graph TD\n    A[Correlated Features X1, X2, X3] --&gt; B[Covariance Matrix]\n    B --&gt; C[Eigen Decomposition]\n    C --&gt; D[Principal Components Z1, Z2, ...]</code></pre>"},{"location":"Chapter12.html#42-mathematical-steps","title":"4.2 Mathematical Steps","text":"<p>Given data matrix \\( X \\) (n samples \u00d7 p features):</p> <ol> <li>Standardize data: \\(x'_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\\)</li> <li>Compute covariance matrix: \\(C = \\frac{1}{n-1} X^T X\\)</li> <li>Find eigenvalues and eigenvectors: \\(C v_i = \\lambda_i v_i\\)</li> <li>Sort eigenvalues in descending order \u2192 select top k components.</li> <li>Project data: \\(Z = X V_k\\)</li> </ol> <p>where \\(V_k\\) contains eigenvectors of top k eigenvalues.</p>"},{"location":"Chapter12.html#43-example-by-hand","title":"4.3 Example (by hand)","text":"<p>Data (2D): </p> X1 X2 2.5 2.4 0.5 0.7 2.2 2.9 1.9 2.2 3.1 3.0 2.3 2.7 2.0 1.6 1.0 1.1 1.5 1.6 1.1 0.9 <p>Covariance matrix: $$ C = \\begin{bmatrix} 0.6166 &amp; 0.6154 \\ 0.6154 &amp; 0.7166 \\end{bmatrix} $$</p> <p>Eigenvalues: 1.284, 0.049 \u2192 First principal component captures 96% variance. \u2192 One dimension (Z1) is enough to represent data.</p>"},{"location":"Chapter12.html#5-attribute-subset-selection","title":"\ud83e\udde0 5. Attribute Subset Selection","text":""},{"location":"Chapter12.html#definition_1","title":"Definition","text":"<p>Select the most relevant attributes to preserve predictive power while reducing redundancy.</p>"},{"location":"Chapter12.html#51-methods","title":"5.1 Methods","text":"Method Description Example Filter Use statistical tests independent of model Correlation, Chi\u2011square Wrapper Evaluate subsets using model performance Forward/backward selection Embedded Feature selection occurs during training LASSO, Decision tree importance <pre><code>graph TD\n    A[All Attributes] --&gt; B[Filter Method]\n    A --&gt; C[Wrapper Method]\n    A --&gt; D[Embedded Method]\n    B &amp; C &amp; D --&gt; E[Selected Subset]</code></pre>"},{"location":"Chapter12.html#52-example-backward-elimination","title":"5.2 Example \u2014 Backward Elimination","text":"<ol> <li>Start with all features: F1, F2, F3, F4  </li> <li>Remove one with least contribution to model accuracy  </li> <li>Repeat until no improvement</li> </ol>"},{"location":"Chapter12.html#6-comparison-of-reduction-techniques","title":"\ud83e\udde9 6. Comparison of Reduction Techniques","text":"Technique Purpose Output Example Tool Wavelet Transform Compress numeric data Coefficients MATLAB, PyWavelets PCA Reduce correlated features Principal components scikit\u2011learn Attribute Subset Selection Choose best predictors Selected columns Weka, Python sklearn"},{"location":"Chapter12.html#7-practice-questions","title":"\ud83d\udcd8 7. Practice Questions","text":"<ol> <li>Explain how wavelet transformation achieves data compression. </li> <li>Derive the PCA transformation mathematically. </li> <li>Distinguish between numerosity reduction and dimensionality reduction. </li> <li>Compare filter, wrapper, and embedded feature selection methods. </li> <li>Compute the first principal component for a 2D dataset manually. </li> </ol>"},{"location":"Chapter13.html","title":"\ud83d\udd04 Chapter 13 \u2014 Data Transformation and Discretization","text":"<p>This chapter explores data transformation strategies, normalization, discretization by binning, and concept hierarchy generation for nominal data.</p>"},{"location":"Chapter13.html#1-overview-of-data-transformation","title":"\u2699\ufe0f 1. Overview of Data Transformation","text":"<p>Data Transformation converts data into an appropriate format or structure for analysis. It improves data quality and makes algorithms more efficient and comparable.</p> <pre><code>graph TD\n    A[Raw Data] --&gt; B[Data Transformation]\n    B --&gt; C[Normalized Data]\n    B --&gt; D[Discretized Data]\n    B --&gt; E[Hierarchical Categories]</code></pre>"},{"location":"Chapter13.html#objectives","title":"Objectives","text":"<ul> <li>Scale numeric data into comparable ranges  </li> <li>Reduce noise and variation  </li> <li>Improve model convergence  </li> <li>Simplify representation (e.g., categorical grouping)</li> </ul>"},{"location":"Chapter13.html#2-data-transformation-strategies","title":"\ud83d\udccf 2. Data Transformation Strategies","text":"Transformation Type Description Example Smoothing Remove noise from data Moving averages Aggregation Summarize values Weekly \u2192 monthly sales Generalization Replace detailed data with higher-level concepts \u201cStudent Age\u201d \u2192 \u201cAge Group\u201d Normalization Scale data to fixed range [0,1] scaling Discretization Convert continuous to categorical Age \u2192 Young, Middle, Old Attribute Construction Create new derived features BMI = weight/height\u00b2 <pre><code>graph LR\n    A[Raw Attributes] --&gt; B[Transformation Methods]\n    B --&gt; C[Normalized Values]\n    B --&gt; D[Discretized Intervals]\n    B --&gt; E[Constructed Attributes]</code></pre>"},{"location":"Chapter13.html#3-data-transformation-by-normalization","title":"\ud83d\udcc9 3. Data Transformation by Normalization","text":""},{"location":"Chapter13.html#definition","title":"Definition","text":"<p>Normalization adjusts numeric data to a common scale without distorting differences in ranges or distributions.</p>"},{"location":"Chapter13.html#31-minmax-normalization","title":"3.1 Min\u2013Max Normalization","text":"<p>Maps data into the range [0,1]:  </p> \\[ x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\] <p>Example: Given values [10, 15, 20, 25], normalize 20.</p> \\[x' = \\frac{20 - 10}{25 - 10} = \\frac{10}{15} = 0.667\\] Original Normalized 10 0.0 15 0.33 20 0.67 25 1.0"},{"location":"Chapter13.html#32-zscore-normalization","title":"3.2 Z\u2011Score Normalization","text":"<p>Standardizes data using mean (\u03bc) and standard deviation (\u03c3):  </p> \\[ x' = \\frac{x - \\mu}{\\sigma} \\] <p>Example: x = 50, \u03bc = 40, \u03c3 = 5 \u2192 \\( x' = (50 - 40) / 5 = 2.0 \\)</p> Original Z\u2011score 30 \u22122 40 0 50 +2"},{"location":"Chapter13.html#33-decimal-scaling-normalization","title":"3.3 Decimal Scaling Normalization","text":"<p>Moves the decimal point of values to bring them within [\u22121,1]:  </p> \\[ x' = \\frac{x}{10^j}\\] <p>where j = smallest integer such that max(|x'|) &lt; 1</p> <p>Example: x = 345 \u2192 \\( x' = 0.345 \\) since j = 3</p>"},{"location":"Chapter13.html#4-discretization-by-binning","title":"\ud83d\udd22 4. Discretization by Binning","text":""},{"location":"Chapter13.html#definition_1","title":"Definition","text":"<p>Discretization converts continuous attributes into categorical intervals (bins). It helps reduce small observation effects and simplify patterns.</p>"},{"location":"Chapter13.html#41-types-of-binning","title":"4.1 Types of Binning","text":"Type Description Example Equal\u2011width Divide range into equal intervals 0\u201310, 10\u201320, 20\u201330 Equal\u2011frequency Each bin has equal number of samples 5 per bin Supervised Bins chosen based on class labels Decision tree splits"},{"location":"Chapter13.html#42-example-equalwidth-binning","title":"4.2 Example \u2014 Equal\u2011width Binning","text":"<p>Data: [4, 8, 15, 21, 24, 25, 28, 34, 35, 40]</p> <p>Range = 40 \u2212 4 = 36 If 3 bins \u2192 Bin width = 36 / 3 = 12</p> Bin Interval Values 1 [4\u201316) 4, 8, 15 2 [16\u201328) 21, 24, 25 3 [28\u201340] 28, 34, 35, 40 <pre><code>graph TD\n    A[Continuous Data] --&gt; B[Equal-width Intervals]\n    B --&gt; C[Bin 1: 4-16]\n    B --&gt; D[Bin 2: 16-28]\n    B --&gt; E[Bin 3: 28-40]</code></pre>"},{"location":"Chapter13.html#43-smoothing-by-binning","title":"4.3 Smoothing by Binning","text":"<p>Within each bin, replace values by: - Bin mean (average) - Bin median - Bin boundary values</p> <p>Example (Bin 1 = [4, 8, 15]) \u2192 Mean smoothing: replace with 9 (average)</p> <p>Smoothed data: [9, 9, 9, 23, 23, 23, 34, 34, 34, 34]</p>"},{"location":"Chapter13.html#5-concept-hierarchy-generation-for-nominal-data","title":"\ud83e\udded 5. Concept Hierarchy Generation for Nominal Data","text":""},{"location":"Chapter13.html#definition_2","title":"Definition","text":"<p>For categorical attributes, data can be organized into concept hierarchies that represent levels of abstraction.</p> <pre><code>graph TD\n    A[City] --&gt; B[State]\n    B --&gt; C[Country]\n    C --&gt; D[Continent]</code></pre>"},{"location":"Chapter13.html#51-example-location-hierarchy","title":"5.1 Example \u2014 Location Hierarchy","text":"Level Example City Hyderabad State Telangana Country India Continent Asia"},{"location":"Chapter13.html#52-hierarchy-generation-methods","title":"5.2 Hierarchy Generation Methods","text":"Approach Description Example Explicit specification Defined by domain expert City \u2192 State \u2192 Country Automatic generation Derived from data attributes ZIP \u2192 City \u2192 Region Schema-based Based on database schema Employee_ID \u2192 Department \u2192 Division"},{"location":"Chapter13.html#6-summary-table","title":"\ud83e\udde0 6. Summary Table","text":"Process Purpose Techniques Transformation Adjust data scale or distribution Normalization, smoothing Normalization Scale numeric attributes Min\u2013max, Z\u2011score, Decimal scaling Discretization Convert numeric to categorical Binning, decision tree splits Concept Hierarchy Abstract categorical data City\u2192State\u2192Country"},{"location":"Chapter13.html#7-practice-questions","title":"\ud83d\udcd8 7. Practice Questions","text":"<ol> <li>Explain the importance of data transformation before modeling. </li> <li>Derive the formula for Z\u2011score normalization and compute for x=70, \u03bc=50, \u03c3=10. </li> <li>Perform equal\u2011width binning for the dataset [3,5,7,9,11,13,15] into 2 bins. </li> <li>Differentiate between equal\u2011width and equal\u2011frequency discretization. </li> <li>Draw a concept hierarchy for \u201cProduct Category\u201d (e.g., Item \u2192 Type \u2192 Department \u2192 Store). </li> </ol>"},{"location":"Chapter14.html","title":"\ud83d\udcd8 Chapter 14 \u2014 Regression","text":"<p>Goal: Understand Linear Regression and Logistic Regression (for classification), and master core regression metrics: MAE, MSE, RMSE, R\u00b2. Includes theory, math, by-hand examples.</p>"},{"location":"Chapter14.html#1-linear-regression","title":"1) Linear Regression","text":""},{"location":"Chapter14.html#11-problem-setup","title":"1.1 Problem Setup","text":"<p>Given data points \\((x_i, y_i)\\) for \\(i=1,\\dots,n\\), fit a line $$ \\hat y_i = \\beta_0 + \\beta_1 x_i $$ that minimizes the sum of squared errors (SSE): $$ \\text{SSE}(\\beta_0,\\beta_1) = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2 . $$</p>"},{"location":"Chapter14.html#12-closed-form-normal-equations","title":"1.2 Closed-Form (Normal Equations)","text":"<p>Let \\(\\bar x\\) and \\(\\bar y\\) be sample means. $$ \\hat\\beta_1 = \\frac{\\sum_i (x_i-\\bar x)(y_i-\\bar y)}{\\sum_i (x_i-\\bar x)^2}, \\qquad \\hat\\beta_0 = \\bar y - \\hat\\beta_1 \\bar x . $$</p> <p>Matrix form: with \\(X = [\\,x\\ \\ 1\\,]\\) and \\(y \\in \\mathbb{R}^n\\), $$ \\hat{\\boldsymbol\\beta} = (X^\\top X)^{-1} X^\\top y,\\quad \\boldsymbol\\beta = \\begin{bmatrix}\\beta_1\\ \\beta_0\\end{bmatrix}. $$</p>"},{"location":"Chapter14.html#13-by-hand-example-tiny-dataset","title":"1.3 By-Hand Example (tiny dataset)","text":"<p>Data: \\((x,y) \\in \\{(1,2),(2,3),(3,5),(4,4),(5,7)\\}\\). Compute means: \\(\\bar x=3,\\ \\bar y=4.2\\).</p> <p>\\(\\sum (x-\\bar x)(y-\\bar y) = 11.0\\) and \\(\\sum (x-\\bar x)^2=10\\). So \\(\\hat\\beta_1 = 11/10 = 1.1\\). \\(\\hat\\beta_0 = 4.2 - 1.1\\cdot 3 = 0.9\\). Fitted line: \\(\\hat y = 0.9 + 1.1x\\).</p> <p>Now compute predictions and residuals:</p> \\[ \\begin{array}{c|ccccc} x &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\\\hline y &amp; 2 &amp; 3 &amp; 5 &amp; 4 &amp; 7 \\\\ \\hat y &amp; 2.0 &amp; 3.1 &amp; 4.2 &amp; 5.3 &amp; 6.4 \\\\ e=y-\\hat y &amp; 0.0 &amp; -0.1 &amp; 0.8 &amp; -1.3 &amp; 0.6 \\end{array} \\]"},{"location":"Chapter14.html#14-visuals","title":"1.4 Visuals","text":""},{"location":"Chapter14.html#2-logistic-regression-for-classification","title":"2) Logistic Regression (for Classification)","text":""},{"location":"Chapter14.html#21-model","title":"2.1 Model","text":"<p>For binary \\(y\\in\\{0,1\\}\\), logistic regression models the log-odds via a linear function: $$ \\log\\frac{p(y=1\\mid \\mathbf x)}{1-p(y=1\\mid \\mathbf x)} = \\mathbf w^\\top \\mathbf x + b, $$ so that $$ p(y=1\\mid \\mathbf x) = \\sigma(\\mathbf w^\\top \\mathbf x + b) = \\frac{1}{1+e^{-(\\mathbf w^\\top \\mathbf x + b)}} . $$ Decision rule (default): predict \\(1\\) if \\(p \\ge 0.5\\), else \\(0\\).</p>"},{"location":"Chapter14.html#22-maximum-likelihood-loss","title":"2.2 Maximum Likelihood &amp; Loss","text":"<p>Given data \\(\\{(\\mathbf x_i,y_i)\\}\\), the (negative) log-likelihood $$ \\mathcal L(\\mathbf w,b)= -\\sum_{i=1}^n \\Big[y_i\\log \\sigma(z_i) + (1-y_i)\\log(1-\\sigma(z_i))\\Big],\\quad z_i=\\mathbf w^\\top \\mathbf x_i + b. $$ Minimize \\(\\mathcal L\\) using gradient-based optimization.</p>"},{"location":"Chapter14.html#23-visual","title":"2.3 Visual","text":""},{"location":"Chapter14.html#3-regression-metrics","title":"3) Regression Metrics","text":"<p>Given true values \\(y_i\\) and predictions \\(\\hat y_i\\):</p>"},{"location":"Chapter14.html#31-mean-absolute-error-mae","title":"3.1 Mean Absolute Error (MAE)","text":"\\[ \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat y_i| . \\]"},{"location":"Chapter14.html#32-mean-squared-error-mse","title":"3.2 Mean Squared Error (MSE)","text":"\\[ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat y_i)^2 . \\]"},{"location":"Chapter14.html#33-root-mean-squared-error-rmse","title":"3.3 Root Mean Squared Error (RMSE)","text":"\\[ \\text{RMSE} = \\sqrt{\\text{MSE}} . \\]"},{"location":"Chapter14.html#34-coefficient-of-determination-r2","title":"3.4 Coefficient of Determination (R\u00b2)","text":"<p>Let \\(\\bar y=\\frac{1}{n}\\sum_i y_i\\). </p> <p>Define \\(\\text{SS}_{{tot}}=\\sum_i (y_i-\\bar y)^2\\) and \\(\\text{SS}_{{res}}=\\sum_i (y_i-\\hat y_i)^2\\).</p> \\[ R^2 = 1 - \\frac{\\text{SS}_{{res}}}{\\text{SS}_{{tot}}} . \\]"},{"location":"Chapter14.html#35-by-hand-metric-example","title":"3.5 By-Hand Metric Example","text":"<p>Using the tiny dataset and the fitted line \\(\\hat y=0.9+1.1x\\): \\(\\begin{array}{c|ccccc} x &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\  \\hline y &amp; 2 &amp; 3 &amp; 5 &amp; 4 &amp; 7 \\\\ \\hat y &amp; 2.0 &amp; 3.1 &amp; 4.2 &amp; 5.3 &amp; 6.4 \\\\ |e| &amp; 0.0 &amp; 0.1 &amp; 0.8 &amp; 1.3 &amp; 0.6 \\\\ e^2 &amp; 0.0 &amp; 0.01 &amp; 0.64 &amp; 1.69 &amp; 0.36 \\end{array}\\)</p> <p>\\(\\text{MAE}=(0+0.1+0.8+1.3+0.6)/5=0.56\\). \\(\\text{MSE}=(0+0.01+0.64+1.69+0.36)/5=0.54\\). \\(\\text{RMSE}=\\sqrt{0.54}=0.735\\). \\(\\bar y=4.2\\Rightarrow \\text{SS}_{{tot}}=14.8\\), \\(\\text{SS}_{{res}}=5\\cdot 0.54=2.7\\). \\(R^2=1-2.7/14.8=0.8189\\) (\u2248 0.819).</p>"},{"location":"Chapter14.html#4-practice-questions","title":"4) Practice Questions","text":"<ol> <li>Derive the OLS solution \\((X^\\top X)^{-1}X^\\top y\\) starting from SSE.  </li> <li>Why is logistic regression optimized by maximum likelihood rather than least squares?  </li> <li>Show that minimizing MSE is equivalent to maximizing the Gaussian likelihood (with fixed variance).  </li> <li>Given \\(\\text{SS}_{{tot}}=100\\) and \\(\\text{SS}_{{res}}=35\\), compute \\(R^2\\).  </li> <li>For the by-hand dataset, recompute MAE/MSE/RMSE if the model is \\(\\hat y=1+1.0x\\).</li> </ol>"},{"location":"Chapter15.html","title":"\ud83e\udd16 Chapter 15 \u2014 Classification","text":"<p>Principles of classification, Confusion Matrix, Precision, Recall, F1, and AUC\u2013ROC with theory, math, a worked example.</p>"},{"location":"Chapter15.html#1-classification-principles","title":"1) Classification Principles","text":"<p>Classification predicts discrete labels by learning a function: $$ f: \\mathbb{R}^n \\to {1,2,\\dots,K},\\qquad f^* = \\arg\\min_f\\; \\mathbb{E}_{(x,y)}[L(y,f(x))]. $$</p> <p>Binary setup uses a score \\(s(x)\\) and threshold \\(\\tau\\): $$ \\hat y = \\begin{cases} 1,&amp; s(x)\\ge \\tau \\ 0,&amp; s(x)&lt;\\tau \\end{cases} $$</p> <pre><code>graph TD\n    A[Features] --&gt; B[Classifier]\n    B --&gt; C[Predicted Label]</code></pre>"},{"location":"Chapter15.html#2-confusion-matrix","title":"2) Confusion Matrix","text":"Predicted + Predicted - Actual + True Positive (TP) False Negative (FN) Actual - False Positive (FP) True Negative (TN) <p>Derived rates: $$ \\text{TPR}=\\frac{TP}{TP+FN}\\quad(\\text{Recall}),\\qquad \\text{FPR}=\\frac{FP}{FP+TN}. $$</p> <p>By\u2011hand example: TP=8, FP=2, TN=7, FN=3 Accuracy=0.75, Precision=0.80, Recall=0.727, F1\u22480.762</p> <p>Visualization (synthetic): </p>"},{"location":"Chapter15.html#3-precision-recall-f1","title":"3) Precision, Recall, F1","text":"\\[ \\text{Precision}=\\frac{TP}{TP+FP},\\quad \\text{Recall}=\\frac{TP}{TP+FN},\\quad F_1=2\\cdot\\frac{PR}{P+R}. \\] <p>When to favor each: Precision (costly FP), Recall (costly FN), F1 (balance).</p>"},{"location":"Chapter15.html#4-roc-curve-and-auc","title":"4) ROC Curve and AUC","text":"<p>Vary \\(\\tau\\) to get points \\((\\text{FPR}(\\tau),\\text{TPR}(\\tau))\\) forming the ROC curve. AUC = area under ROC; probability a random positive is ranked above a random negative.</p> <p>Visualization (synthetic): </p>"},{"location":"Chapter15.html#6-practice-questions","title":"6) Practice Questions","text":"<ol> <li>Derive Precision, Recall, and F1 from the confusion matrix.  </li> <li>Explain why AUC is threshold\u2011independent.  </li> <li>Compare ROC and Precision\u2013Recall curves.  </li> <li>Why can accuracy be misleading for imbalanced datasets?  </li> <li>Show how moving the decision threshold affects Precision and Recall.</li> </ol>"},{"location":"Chapter16.html","title":"\ud83e\udde9 Chapter 16 \u2014 Clustering","text":"<p>Principles of clustering, k-Means algorithm (math + by-hand example), and evaluation metrics (SSE/Inertia, Silhouette, Davies\u2013Bouldin).</p>"},{"location":"Chapter16.html#1-clustering-principles","title":"1) Clustering Principles","text":"<p>Goal: Group data into clusters so that within-cluster points are similar and between-cluster points are dissimilar.</p> <pre><code>graph TD\n    A[Unlabeled Data] --&gt; B[Clustering Algorithm]\n    B --&gt; C[Clusters]</code></pre>"},{"location":"Chapter16.html#similarity-distance","title":"Similarity / Distance","text":"<p>Common choice for numeric data is Euclidean distance:</p> \\[ d(\\mathbf x_i,\\mathbf x_j)=\\lVert \\mathbf x_i-\\mathbf x_j\\rVert_2  = \\sqrt{ \\sum_{p=1}^P (x_{ip}-x_{jp})^2 } . \\]"},{"location":"Chapter16.html#2-k-means-objective-and-algorithm","title":"2) k-Means: Objective and Algorithm","text":""},{"location":"Chapter16.html#21-objective","title":"2.1 Objective","text":"<p>Given data \\(X=\\{\\mathbf x_i\\}_{i=1}^n\\), partition into \\(k\\) clusters with centers \\( \\{\\boldsymbol\\mu_c\\}_{c=1}^k \\) by minimizing:</p> \\[ \\text{SSE} = \\sum_{c=1}^k \\ \\sum_{\\mathbf x_i \\in C_c} \\lVert \\mathbf x_i - \\boldsymbol\\mu_c \\rVert^2. \\]"},{"location":"Chapter16.html#22-alternating-minimization","title":"2.2 Alternating Minimization","text":"<ul> <li>Assignment step: assign each point to nearest center \\( r_{ic} = 1 \\) if \\( c = \\arg\\min_j \\lVert \\mathbf x_i - \\boldsymbol\\mu_j \\rVert \\), else \\(0\\).</li> <li>Update step: update each center to the mean of its assigned points \\( \\boldsymbol\\mu_c = \\frac{1}{|C_c|}\\sum_{\\mathbf x_i\\in C_c}\\mathbf x_i \\).</li> </ul> <p>Repeat until assignments stop changing or SSE converges.</p> <pre><code>  flowchart TD\n      S[Start: choose k, init centers] --&gt; A[Assign points to nearest center]\n      A --&gt; B[Update centers = mean of assigned points]\n      B --&gt; C{{Converged?}}\n      C -- No --&gt; A\n      C -- Yes --&gt; E[Return clusters + centers]</code></pre>"},{"location":"Chapter16.html#23-byhand-mini-example-2d-k2","title":"2.3 By\u2011hand Mini Example (2D, k=2)","text":"<p>Points: \\( (0,0),(1,0),(0,1), (5,5),(6,5),(5,6) \\). Init centers: \\( \\mu_1=(0,0), \\mu_2=(5,5) \\). - Assign first 3 near \\( \\mu_1 \\), last 3 near \\( \\mu_2 \\). - Update: \\( \\mu_1=(\\tfrac{1}{3},\\tfrac{1}{3}), \\mu_2=(\\tfrac{16}{3},\\tfrac{16}{3}) \\). - Reassign \u2192 unchanged \u21d2 converged.</p>"},{"location":"Chapter16.html#3-evaluation-parameters-metrics","title":"3) Evaluation Parameters &amp; Metrics","text":""},{"location":"Chapter16.html#31-sse-inertia","title":"3.1 SSE / Inertia","text":"<p>\\( \\text{SSE} \\) is the within-cluster sum of squared distances. Lower is better (for fixed \\(k\\)). Used by the Elbow method: plot SSE vs \\(k\\) and look for a \u201cknee\u201d where marginal gain drops.</p>"},{"location":"Chapter16.html#32-silhouette-coefficient","title":"3.2 Silhouette Coefficient","text":"<p>For point \\(i\\): \\( a(i) \\) = average distance to points in its own cluster; \\( b(i) \\) = minimum average distance to other clusters. Silhouette: $$ s(i)=\\frac{b(i)-a(i)}{\\max{a(i),\\,b(i)}}\\in[-1,1]. $$ Overall score is the mean \\(s(i)\\). Higher is better.</p>"},{"location":"Chapter16.html#33-daviesbouldin-db-index","title":"3.3 Davies\u2013Bouldin (DB) Index","text":"<p>For cluster \\(c\\), let \\(S_c\\) be average distance of points in \\(c\\) to center \\(\\mu_c\\); and \\(M_{cd}=\\lVert \\mu_c-\\mu_d\\rVert\\). For each \\(c\\), compute \\(R_{cd}=\\frac{S_c+S_d}{M_{cd}}\\) for \\(d\\ne c\\), then</p> <p>$$ \\text{DB}=\\frac{1}{k}\\sum_{c=1}^k \\max_{d\\ne c} R_{cd}. $$ Lower DB is better (compact &amp; well-separated clusters).</p>"},{"location":"Chapter16.html#4-visualizations-synthetic-2d","title":"4) Visualizations (Synthetic 2\u2011D)","text":""},{"location":"Chapter16.html#41-k-means-clusters-k3","title":"4.1 k-Means Clusters (k=3)","text":""},{"location":"Chapter16.html#42-elbow-method","title":"4.2 Elbow Method","text":""},{"location":"Chapter16.html#6-practice-questions","title":"6) Practice Questions","text":"<ol> <li>Show that the k-Means center update is the mean of assigned points by minimizing SSE.  </li> <li>For the by-hand two\u2011cluster example, compute SSE after convergence.  </li> <li>Compare the effect of Euclidean vs Manhattan distance for k-Means assignments.  </li> <li>Why can Silhouette be misleading for non\u2011convex clusters? Suggest an alternative.  </li> <li>Use the Elbow and Silhouette methods together to choose \\(k\\).</li> </ol>"},{"location":"Chapter17.html","title":"\ud83c\udfaf Chapter 17 \u2014 Model Evaluation","text":"<p>Generalization error, out-of-sample evaluation, overfitting vs underfitting, cross-validation, and metrics for both regression and classification. Includes math, worked examples, Mermaid diagrams.</p>"},{"location":"Chapter17.html#1-generalization-error-out-of-sample-evaluation","title":"1) Generalization Error &amp; Out-of-Sample Evaluation","text":"<p>Goal: Estimate how well a model will perform on unseen data.</p> <p>Let data distribution be \\(\\mathcal{D}\\) and loss \\(L\\). The generalization error of a learned predictor \\(\\hat f\\) is:</p> \\[\\mathcal{E}_{\\text{gen}}(\\hat f) = \\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[L(y, \\hat f(x))]. \\] <p>Since \\(\\mathcal{D}\\) is unknown, we estimate with a test set:</p> \\[\\hat{\\mathcal{E}}_{\\text{test}}(\\hat f) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_i, \\hat f(x_i)). \\] <pre><code>graph TD\n    A[Dataset] --&gt; B[Train Set]\n    A --&gt; C[Validation Set]\n    A --&gt; D[Test Set]\n    B --&gt; E[Train Model]\n    C --&gt; F[Tune Hyperparameters]\n    D --&gt; G[Final Out-of-Sample Estimate]</code></pre>"},{"location":"Chapter17.html#2-biasvariance-tradeoff-regression","title":"2) Bias\u2013Variance Tradeoff (Regression)","text":"<p>For squared loss:</p> \\[\\mathbb E[(y-\\hat f(x))^2] = (\\mathbb E[\\hat f(x)] - f(x))^2 + \\mathbb Var[\\hat f(x)] + \\sigma^2. \\] <ul> <li>Underfitting \u2192 high bias, low variance (too simple).  </li> <li>Overfitting \u2192 low bias, high variance (too complex).</li> </ul> <p>Visualization: </p>"},{"location":"Chapter17.html#3-evaluation-metrics","title":"3) Evaluation Metrics","text":""},{"location":"Chapter17.html#31-regression-metrics","title":"3.1 Regression Metrics","text":"Metric Formula Interpretation MAE \\(\\frac{1}{n}\\sum_i\\|y_i - \\hat y_i\\|\\) Average absolute error MSE \\(\\frac{1}{n}\\sum_i (y_i - \\hat y_i)^2\\) Penalizes large errors RMSE \\(\\sqrt{\\text{MSE}} \\) Root of MSE R\u00b2 \\( 1 - \\frac{\\sum_i (y_i-\\hat y_i)^2}{\\sum_i (y_i-\\bar y)^2}\\) Variance explained <p>Manual example (tiny): True: [2, 3, 5, 4, 7], Pred: [2.0, 3.1, 4.2, 5.3, 6.4] \u2192 MAE=0.56, MSE=0.54, RMSE\u22480.73, R\u00b2\u22480.82</p>"},{"location":"Chapter17.html#32-classification-metrics","title":"3.2 Classification Metrics","text":"<p>Confusion matrix (binary):</p> Predicted + Predicted - Actual + TP FN Actual - FP TN <ul> <li>Accuracy: \\( \\frac{TP+TN}{TP+FP+TN+FN} \\) </li> <li>Precision: \\( \\frac{TP}{TP+FP} \\) </li> <li>Recall (TPR): \\( \\frac{TP}{TP+FN} \\) </li> <li>F1: \\( 2\\cdot \\frac{PR}{P+R} \\) </li> <li>AUC: area under ROC curve (threshold-independent).</li> </ul>"},{"location":"Chapter17.html#4-cross-validation","title":"4) Cross-Validation","text":"<p>k\u2011Fold Cross Validation: Divide data into k folds; train on k\u22121 and validate on the remaining fold.</p> \\[\\hat S_{\\text{CV}} = \\frac{1}{k}\\sum_{j=1}^k S(\\hat f^{(-j)}, V_j) \\] <ul> <li>Stratified CV: keeps class balance (classification).  </li> <li>LOOCV: each fold has one sample; high variance.</li> </ul> <p>Visualization: </p>"},{"location":"Chapter17.html#5-overfitting-vs-underfitting-quick-guide","title":"5) Overfitting vs Underfitting \u2013 Quick Guide","text":"Situation Train Error Validation Error Description Underfitting High High Model too simple Overfitting Low High Model too complex Good Fit Low Low Balanced generalization"},{"location":"Chapter17.html#6-practice-questions","title":"6) Practice Questions","text":"<ol> <li>Derive the bias\u2013variance decomposition for squared loss.  </li> <li>Explain why test data must remain untouched until final evaluation.  </li> <li>Demonstrate k-fold CV on small data by hand.  </li> <li>Compute Accuracy, Precision, Recall, and F1 from a confusion matrix.  </li> <li>Explain how cross-validation mitigates overfitting.</li> </ol>"},{"location":"Chapter2.html","title":"\ud83d\udcd8 Chapter 2 \u2014 What is Data Science","text":"<p>Data Science is the discipline that integrates statistics, machine learning, and computing to extract knowledge and insights from data.</p>"},{"location":"Chapter2.html#1-definition-of-data-science","title":"1. Definition of Data Science","text":"<p>Data Science is the study of methods and systems to extract meaningful patterns and actionable insights from structured and unstructured data. It unites principles from mathematics, statistics, information theory, and computer science.</p> <p>In simple form, Data Science seeks a mapping:</p> \\[ f: X \\rightarrow Y \\] <p>where: - \\(X\\) = input or feature space - \\(Y\\) = target or outcome - \\(f\\) = learned model or pattern - \\(\\varepsilon\\) = random error or noise</p>"},{"location":"Chapter2.html#2-extracting-meaningful-patterns","title":"2. Extracting Meaningful Patterns","text":"<p>Data scientists explore and analyze large datasets to uncover relationships, trends, and anomalies. Examples include: - Detecting fraudulent credit card transactions - Identifying customer segments in marketing - Finding associations between genes and diseases</p> <p>The process involves: 1. Collecting and cleaning data 2. Identifying variables and correlations 3. Building models that generalize from examples</p> \\[ Y = f(X) + \\varepsilon \\]"},{"location":"Chapter2.html#3-building-representative-models","title":"3. Building Representative Models","text":"<p>A representative model captures essential relationships within the data without memorizing noise. The goal is to generalize \u2014 to perform well not only on training data but also on unseen data.</p> <p>Steps in model building: 1. Define the problem 2. Select appropriate algorithms 3. Train and validate the model 4. Evaluate with metrics such as accuracy, MSE, or AUC  </p>"},{"location":"Chapter2.html#4-combination-of-disciplines","title":"4. Combination of Disciplines","text":"<p>Data Science combines three core disciplines that together enable data-driven decision-making.</p> <p></p> Discipline Role Statistics Understanding data distribution, inference, hypothesis testing Machine Learning Building predictive and adaptive models Computing Efficient data storage, processing, and algorithmic implementation"},{"location":"Chapter2.html#5-learning-algorithms","title":"5. Learning Algorithms","text":"<p>Learning algorithms form the heart of Data Science. They can be grouped into three major categories:</p> <p></p>"},{"location":"Chapter2.html#supervised-learning","title":"\ud83e\udde9 Supervised Learning","text":"<ul> <li>Learns from labeled data (known input\u2013output pairs)  </li> <li>Goal: Predict new outputs for unseen inputs  </li> <li>Examples: </li> <li>Spam detection  </li> <li>Medical diagnosis  </li> <li>Sentiment classification  </li> </ul>"},{"location":"Chapter2.html#unsupervised-learning","title":"\ud83d\udd0d Unsupervised Learning","text":"<ul> <li>Works on unlabeled data to find hidden structure  </li> <li>Goal: Discover natural patterns or clusters  </li> <li>Examples: </li> <li>Customer segmentation  </li> <li>Topic modeling  </li> <li>Anomaly detection  </li> </ul>"},{"location":"Chapter2.html#reinforcement-learning","title":"\ud83c\udfae Reinforcement Learning","text":"<ul> <li>Learns by interacting with an environment and receiving feedback (rewards)  </li> <li>Goal: Learn optimal actions over time  </li> <li>Examples: </li> <li>Game-playing AI  </li> <li>Autonomous robots  </li> <li>Recommendation tuning  </li> </ul>"},{"location":"Chapter2.html#6-associated-fields","title":"6. Associated Fields","text":"<p>Data Science intersects with several related areas that extend its capabilities.</p> <p></p> Field Connection to Data Science Artificial Intelligence Broader goal of building intelligent systems Data Mining Focuses on pattern discovery in large datasets Big Data Analytics Handles high-volume, high-velocity data streams Cloud Computing Provides scalable data storage and computation Data Visualization Transforms analysis results into human-interpretable visuals"},{"location":"Chapter2.html#7-key-takeaways","title":"7. Key Takeaways","text":"<ul> <li>Data Science integrates mathematics, statistics, and computing for data-driven discovery.  </li> <li>The process involves extracting patterns, building models, and deploying insights.  </li> <li>It relies on learning algorithms that can generalize from data.  </li> <li>Data Science is both a science (theory, inference) and an engineering practice (implementation, scalability).  </li> </ul>"},{"location":"Chapter2.html#suggested-reading","title":"\ud83d\udcda Suggested Reading","text":"<ul> <li>The Elements of Statistical Learning \u2014 Hastie, Tibshirani, and Friedman  </li> <li>Data Science for Business \u2014 Provost and Fawcett  </li> <li>Python for Data Analysis \u2014 Wes McKinney  </li> </ul>"},{"location":"Chapter3.html","title":"Chapter 3: Data Objects and Attribute Types","text":""},{"location":"Chapter3.html#what-is-an-attribute","title":"What Is an Attribute?","text":""},{"location":"Chapter3.html#definition","title":"Definition","text":"<p>An attribute (also known as feature, variable, or field) is a data field that represents a characteristic or property of a data object.</p>"},{"location":"Chapter3.html#mathematical-representation","title":"Mathematical Representation","text":"<p>For a dataset with n objects and m attributes, we can represent it as:</p> \\[ D = \\{x_1, x_2, ..., x_n\\} \\] <p>where each object \\(x_i\\) is represented as:</p> \\[ x_i = (x_{i1}, x_{i2}, ..., x_{im}) \\] <p>Here, \\( x_{ij} \\) represents the value of the j-th attribute for the i-th object.</p>"},{"location":"Chapter3.html#key-terminology","title":"Key Terminology","text":"<ul> <li>Data Object: Represents an entity (e.g., a customer, product, transaction)</li> <li>Attribute: Property or characteristic of an object</li> <li>Attribute Value: The actual measurement or observation</li> </ul> <pre><code>graph TD\n    A[Data Object] --&gt; B[Attribute 1]\n    A --&gt; C[Attribute 2]\n    A --&gt; D[Attribute 3]\n    A --&gt; E[...]\n\n    B --&gt; B1[Attribute Value]\n    C --&gt; C1[Attribute Value]\n    D --&gt; D1[Attribute Value]</code></pre>"},{"location":"Chapter3.html#nominal-attributes","title":"Nominal Attributes","text":""},{"location":"Chapter3.html#definition_1","title":"Definition","text":"<p>Nominal attributes represent categories or states without any inherent ordering.</p>"},{"location":"Chapter3.html#properties","title":"Properties","text":"<ul> <li>Symbolic values with no numerical significance</li> <li>No ordering between values</li> <li>Only equality operations are meaningful (=, \u2260)</li> <li>Also called categorical or qualitative attributes</li> </ul>"},{"location":"Chapter3.html#mathematical-representation_1","title":"Mathematical Representation","text":"<p>For a nominal attribute A with k possible values:</p> \\[ A \\in \\{v_1, v_2, ..., v_k\\} \\] <p>where the only valid operations are: - \\(v_i = v_j\\) - \\(v_i \\neq v_j\\)</p>"},{"location":"Chapter3.html#examples","title":"Examples","text":"Attribute Possible Values Color {Red, Blue, Green, Yellow} Gender {Male, Female, Other} Country {USA, Canada, UK, Japan} Product Category {Electronics, Clothing, Food}"},{"location":"Chapter3.html#statistical-operations","title":"Statistical Operations","text":"<ul> <li>Valid: Mode, frequency, contingency tables</li> <li>Invalid: Mean, median, standard deviation</li> </ul>"},{"location":"Chapter3.html#binary-attributes","title":"Binary Attributes","text":""},{"location":"Chapter3.html#definition_2","title":"Definition","text":"<p>Binary attributes are a special case of nominal attributes with exactly two possible values.</p>"},{"location":"Chapter3.html#types-of-binary-attributes","title":"Types of Binary Attributes","text":""},{"location":"Chapter3.html#1-symmetric-binary","title":"1. Symmetric Binary","text":"<ul> <li>Both values equally important</li> <li>No preference between outcomes</li> <li>Examples:</li> <li>Gender: {Male, Female}</li> <li>Coin Flip: {Heads, Tails}</li> <li>Switch: {On, Off}</li> </ul>"},{"location":"Chapter3.html#2-asymmetric-binary","title":"2. Asymmetric Binary","text":"<ul> <li>One outcome is more significant or important</li> <li>Typically coded as 1 (presence) and 0 (absence)</li> <li>Examples:</li> <li>Medical Test: {Positive, Negative}</li> <li>Disease: {Yes, No}</li> <li>Feature Presence: {1, 0}</li> </ul>"},{"location":"Chapter3.html#mathematical-representation_2","title":"Mathematical Representation","text":"\\[ A \\in \\{0, 1\\} \\text{ or } A \\in \\{\\text{false}, \\text{true}\\} \\]"},{"location":"Chapter3.html#ordinal-attributes","title":"Ordinal Attributes","text":""},{"location":"Chapter3.html#definition_3","title":"Definition","text":"<p>Ordinal attributes have a meaningful order or ranking among values, but the differences between values are not quantifiable.</p>"},{"location":"Chapter3.html#properties_1","title":"Properties","text":"<ul> <li>Order matters but intervals are not consistent</li> <li>Comparison operations are meaningful (&lt;, &gt;, =)</li> <li>Differences between values cannot be measured precisely</li> </ul>"},{"location":"Chapter3.html#mathematical-representation_3","title":"Mathematical Representation","text":"<p>For an ordinal attribute A:</p> \\[v_1 \\prec v_2 \\prec ... \\prec v_n \\] <p>where \\(\\prec\\) represents the ordering relationship.</p>"},{"location":"Chapter3.html#examples_1","title":"Examples","text":"Attribute Ordered Values Education Level {High School \u227a Bachelor's \u227a Master's \u227a PhD} Customer Rating {Poor \u227a Fair \u227a Good \u227a Very Good \u227a Excellent} Size {Small \u227a Medium \u227a Large \u227a X-Large} Economic Status {Low \u227a Middle \u227a High}"},{"location":"Chapter3.html#statistical-operations_1","title":"Statistical Operations","text":"<ul> <li>Valid: Mode, median, percentiles, rank correlation</li> <li>Invalid: Mean, standard deviation (intervals not consistent)</li> </ul> <pre><code>graph LR\n    A[Ordinal Scale] --&gt; B[Order is Meaningful]\n    A --&gt; C[Intervals Not Consistent]\n    B --&gt; D[Median Valid]\n    C --&gt; E[Mean Invalid]</code></pre>"},{"location":"Chapter3.html#numeric-attributes","title":"Numeric Attributes","text":""},{"location":"Chapter3.html#definition_4","title":"Definition","text":"<p>Numeric attributes are quantitative and represent measurable quantities.</p>"},{"location":"Chapter3.html#types-of-numeric-attributes","title":"Types of Numeric Attributes","text":""},{"location":"Chapter3.html#1-interval-scaled","title":"1. Interval-Scaled","text":"<ul> <li>Measured on a scale with equal units</li> <li>No true zero point</li> <li>Differences are meaningful, ratios are not</li> <li>Examples:</li> <li>Temperature in \u00b0C or \u00b0F</li> <li>Calendar years</li> <li>IQ scores</li> <li>Operations:</li> <li>Differences are meaningful: 100\u00b0C - 90\u00b0C = 10\u00b0C</li> <li>Ratios are not: 100\u00b0C is not twice as hot as 50\u00b0C</li> </ul>"},{"location":"Chapter3.html#2-ratio-scaled","title":"2. Ratio-Scaled","text":"<ul> <li>Has a true zero point</li> <li>Both differences and ratios are meaningful</li> <li>Examples:</li> <li>Height, Weight, Age</li> <li>Income, Sales</li> <li>Counts, Duration</li> <li>Operations:</li> <li>100kg is twice as heavy as 50kg</li> <li>0kg means complete absence of weight</li> </ul>"},{"location":"Chapter3.html#mathematical-properties","title":"Mathematical Properties","text":"<p>For ratio-scaled attributes A and B:</p> <ul> <li>A + B is meaningful</li> <li>A - B is meaningful</li> <li>A \u00d7 B is meaningful</li> <li>A / B is meaningful</li> </ul>"},{"location":"Chapter3.html#discrete-vs-continuous-attributes","title":"Discrete vs Continuous Attributes","text":""},{"location":"Chapter3.html#discrete-attributes","title":"Discrete Attributes","text":""},{"location":"Chapter3.html#definition_5","title":"Definition","text":"<p>Attributes with a finite or countably infinite set of values.</p>"},{"location":"Chapter3.html#characteristics","title":"Characteristics","text":"<ul> <li>Countable set of values</li> <li>Often integer values</li> <li>Gaps between possible values</li> </ul>"},{"location":"Chapter3.html#mathematical-representation-a-subseteq-mathbbz-integers","title":"Mathematical Representation $$ A \\subseteq \\mathbb{Z} (integers) $$","text":""},{"location":"Chapter3.html#examples_2","title":"Examples","text":"<ul> <li>Number of children: {0, 1, 2, 3, ...}</li> <li>Number of cars: {0, 1, 2, ...}</li> <li>Shoe size: {6, 6.5, 7, 7.5, ...}</li> <li>Defect count: {0, 1, 2, ...}</li> </ul>"},{"location":"Chapter3.html#continuous-attributes","title":"Continuous Attributes","text":""},{"location":"Chapter3.html#definition_6","title":"Definition","text":"<p>Attributes with real numbers as values, forming an infinite, uncountable set.</p>"},{"location":"Chapter3.html#characteristics_1","title":"Characteristics","text":"<ul> <li>Measurable quantities</li> <li>Real number values</li> <li>Infinite possible values between any two points</li> </ul>"},{"location":"Chapter3.html#mathematical-representationa-subseteq-mathbbr-text-real-numbers","title":"Mathematical Representation[A \\subseteq \\mathbb{R} \\text{ (real numbers)[","text":""},{"location":"Chapter3.html#examples_3","title":"Examples","text":"<ul> <li>Height: 175.3 cm, 182.7 cm, ...</li> <li>Weight: 68.5 kg, 72.1 kg, ...</li> <li>Temperature: 21.5\u00b0C, 22.3\u00b0C, ...</li> <li>Time: 3.25 seconds, 4.78 seconds, ...</li> </ul> <pre><code>graph TD\n    A[Numeric Attributes] --&gt; B[Discrete]\n    A --&gt; C[Continuous]\n\n    B --&gt; B1[Countable Values]\n    B --&gt; B2[Often Integers]\n    B --&gt; B3[Gaps between values]\n\n    C --&gt; C1[Measurable Quantities]\n    C --&gt; C2[Real Numbers]\n    C --&gt; C3[Infinite possible values]</code></pre>"},{"location":"Chapter3.html#attribute-type-hierarchy","title":"Attribute Type Hierarchy","text":"<pre><code>graph TD\n    A[Attribute Types] --&gt; B[Categorical]\n    A --&gt; C[Numeric]\n\n    B --&gt; D[Nominal]\n    B --&gt; E[Binary]\n    B --&gt; F[Ordinal]\n\n    C --&gt; G[Discrete]\n    C --&gt; H[Continuous]\n\n    H --&gt; I[Interval-Scaled]\n    H --&gt; J[Ratio-Scaled]\n\n    E --&gt; K[Symmetric Binary]\n    E --&gt; L[Asymmetric Binary]</code></pre>"},{"location":"Chapter3.html#summary-table","title":"Summary Table","text":"Attribute Type Ordering Differences Ratios Zero Point Examples Nominal No No No No Color, Gender Binary No No No No Yes/No, On/Off Ordinal Yes No No No Size, Rating Discrete Yes Yes Yes Yes Child count, Shoe size Continuous Interval Yes Yes No No Temperature \u00b0C Continuous Ratio Yes Yes Yes Yes Height, Weight"},{"location":"Chapter3.html#key-mathematical-concepts","title":"Key Mathematical Concepts","text":""},{"location":"Chapter3.html#measurement-scales","title":"Measurement Scales","text":"<p>The hierarchy of measurement scales from weakest to strongest: 1. Nominal - equality only 2. Ordinal - equality + ordering 3. Interval - equality + ordering + differences 4. Ratio - equality + ordering + differences + ratios</p>"},{"location":"Chapter3.html#permissible-statistics","title":"Permissible Statistics","text":"<p>For each attribute type, different statistical operations are meaningful:</p> Operation Nominal Ordinal Interval Ratio Frequency \u2713 \u2713 \u2713 \u2713 Mode \u2713 \u2713 \u2713 \u2713 Median \u2717 \u2713 \u2713 \u2713 Mean \u2717 \u2717 \u2713 \u2713 Standard Deviation \u2717 \u2717 \u2713 \u2713 Ratio \u2717 \u2717 \u2717 \u2713"},{"location":"Chapter3.html#practical-considerations","title":"Practical Considerations","text":""},{"location":"Chapter3.html#data-preprocessing","title":"Data Preprocessing","text":"<p>Understanding attribute types is crucial for: - Encoding: One-hot encoding for nominal, label encoding for ordinal - Normalization: Different techniques for different numeric types - Visualization: Appropriate plots for each attribute type - Analysis: Valid statistical tests and operations</p>"},{"location":"Chapter3.html#machine-learning-implications","title":"Machine Learning Implications","text":"<ul> <li>Nominal: Use one-hot encoding or embedding layers</li> <li>Ordinal: Can use integer encoding with preserved order</li> <li>Numeric: May require scaling/normalization</li> <li>Binary: Can be used directly or with specific loss functions</li> </ul>"},{"location":"Chapter3.html#mini-dataset-example-student-records","title":"Mini Dataset Example \u2014 Student Records","text":""},{"location":"Chapter3.html#dataset-description","title":"Dataset Description","text":"<p>Consider the following student records dataset containing 8 students with various attributes:</p> StudentID Name Age GPA GradeLevel Enrollment Courses Scholarship Attendance Temperature\u00b0C S001 Alice 20 3.75 Junior Full-time 5 Yes 95.2% 22.5 S002 Bob 22 3.20 Senior Part-time 3 No 87.8% 23.1 S003 Charlie 19 2.90 Sophomore Full-time 4 No 92.5% 21.8 S004 Diana 21 3.95 Junior Full-time 5 Yes 98.1% 22.9 S005 Eve 23 3.10 Senior Part-time 3 Yes 85.4% 23.5 S006 Frank 20 2.75 Sophomore Full-time 4 No 89.7% 22.2 S007 Grace 19 3.85 Freshman Full-time 5 Yes 96.3% 21.5 S008 Henry 21 3.45 Junior Part-time 4 No 91.0% 22.7"},{"location":"Chapter3.html#attribute-type-analysis","title":"Attribute Type Analysis","text":""},{"location":"Chapter3.html#1-studentid","title":"1. StudentID","text":"<ul> <li>Type: Nominal</li> <li>Reason: Unique identifier with no mathematical meaning</li> <li>Valid Operations: Equality checks only</li> </ul>"},{"location":"Chapter3.html#2-name","title":"2. Name","text":"<ul> <li>Type: Nominal</li> <li>Reason: Text labels with no inherent order</li> <li>Valid Operations: Equality checks, string matching</li> </ul>"},{"location":"Chapter3.html#3-age","title":"3. Age","text":"<ul> <li>Type: Numeric, Discrete, Ratio-scaled</li> <li>Reason: Whole numbers, true zero point, ratios meaningful</li> <li>Valid Operations: All arithmetic operations</li> <li>Example: 20 years is twice 10 years</li> </ul>"},{"location":"Chapter3.html#4-gpa","title":"4. GPA","text":"<ul> <li>Type: Numeric, Continuous, Interval-scaled</li> <li>Reason: Real numbers, but no true zero (GPA of 0 doesn't mean \"no academic performance\")</li> <li>Valid Operations: Differences meaningful, ratios not meaningful</li> </ul>"},{"location":"Chapter3.html#5-gradelevel","title":"5. GradeLevel","text":"<ul> <li>Type: Ordinal</li> <li>Reason: Ordered categories (Freshman \u227a Sophomore \u227a Junior \u227a Senior)</li> <li>Valid Operations: Comparisons, median, percentiles</li> </ul>"},{"location":"Chapter3.html#6-enrollment","title":"6. Enrollment","text":"<ul> <li>Type: Binary (Symmetric)</li> <li>Reason: Two mutually exclusive categories</li> <li>Valid Operations: Frequency, mode</li> </ul>"},{"location":"Chapter3.html#7-courses","title":"7. Courses","text":"<ul> <li>Type: Numeric, Discrete, Ratio-scaled</li> <li>Reason: Countable integers, true zero point</li> <li>Valid Operations: All arithmetic operations</li> </ul>"},{"location":"Chapter3.html#8-scholarship","title":"8. Scholarship","text":"<ul> <li>Type: Binary (Asymmetric)</li> <li>Reason: Two categories, \"Yes\" is typically more significant</li> <li>Valid Operations: Frequency, mode</li> </ul>"},{"location":"Chapter3.html#9-attendance","title":"9. Attendance","text":"<ul> <li>Type: Numeric, Continuous, Ratio-scaled</li> <li>Reason: Percentage measurements, true zero point (0% means no attendance)</li> <li>Valid Operations: All arithmetic operations</li> </ul>"},{"location":"Chapter3.html#10-temperaturec","title":"10. Temperature\u00b0C","text":"<ul> <li>Type: Numeric, Continuous, Interval-scaled</li> <li>Reason: Temperature measurements, no true zero (0\u00b0C doesn't mean no temperature)</li> <li>Valid Operations: Differences meaningful, ratios not meaningful</li> </ul>"},{"location":"Chapter3.html#visualization-by-attribute-type","title":"Visualization by Attribute Type","text":"<pre><code>graph LR\n    A[Student Dataset] --&gt; B[Nominal]\n    A --&gt; C[Ordinal]\n    A --&gt; D[Binary]\n    A --&gt; E[Numeric Discrete]\n    A --&gt; F[Numeric Continuous]\n\n    B --&gt; B1[StudentID]\n    B --&gt; B2[Name]\n\n    C --&gt; C1[GradeLevel]\n\n    D --&gt; D1[Enrollment]\n    D --&gt; D2[Scholarship]\n\n    E --&gt; E1[Age]\n    E --&gt; E2[Courses]\n\n    F --&gt; F1[GPA]\n    F --&gt; F2[Attendance]\n    F --&gt; F3[Temperature\u00b0C]</code></pre>"},{"location":"Chapter3.html#statistical-analysis-examples","title":"Statistical Analysis Examples","text":""},{"location":"Chapter3.html#for-nominal-attributes-name","title":"For Nominal Attributes (Name):","text":"<ul> <li>Valid: Frequency count of names</li> <li>Invalid: Average name, standard deviation of names</li> </ul>"},{"location":"Chapter3.html#for-ordinal-attributes-gradelevel","title":"For Ordinal Attributes (GradeLevel):","text":"<ul> <li>Valid: Median grade level, mode</li> <li>Invalid: Average grade level (Freshman=1, Sophomore=2, etc.)</li> </ul>"},{"location":"Chapter3.html#for-numeric-ratio-age","title":"For Numeric Ratio (Age):","text":"<ul> <li>Valid: Mean age = 20.6 years, Age range = 19-23 years</li> <li>Valid Ratio: 23 years is approximately 1.21 times 19 years</li> </ul>"},{"location":"Chapter3.html#for-numeric-interval-gpa","title":"For Numeric Interval (GPA):","text":"<ul> <li>Valid: Mean GPA = 3.37, GPA difference between Alice and Bob = 0.55</li> <li>Invalid: Alice's GPA is NOT 1.17 times Bob's GPA</li> </ul>"},{"location":"Chapter3.html#practice-questions","title":"Practice Questions","text":""},{"location":"Chapter3.html#multiple-choice-questions","title":"Multiple Choice Questions","text":"<ol> <li>Which attribute type allows for meaningful calculation of mean and standard deviation?</li> <li>A) Nominal</li> <li>B) Ordinal</li> <li>C) Binary</li> <li>D) Numeric Ratio-scaled</li> </ol> <p>Answer: D</p> <ol> <li>Temperature in Celsius is an example of:</li> <li>A) Nominal attribute</li> <li>B) Ordinal attribute</li> <li>C) Interval-scaled attribute</li> <li>D) Ratio-scaled attribute</li> </ol> <p>Answer: C</p> <ol> <li>Which operation is valid for ordinal attributes but not for nominal attributes?</li> <li>A) Frequency count</li> <li>B) Mode calculation</li> <li>C) Median calculation</li> <li>D) Equality comparison</li> </ol> <p>Answer: C</p>"},{"location":"Chapter3.html#identification-exercises","title":"Identification Exercises","text":"<p>Identify the attribute type for each of the following:</p> <ol> <li>Social Security Number</li> <li> <p>Answer: Nominal (unique identifier)</p> </li> <li> <p>Military Rank (Private, Corporal, Sergeant, Lieutenant)</p> </li> <li> <p>Answer: Ordinal (clear hierarchy)</p> </li> <li> <p>Number of siblings</p> </li> <li> <p>Answer: Numeric, Discrete, Ratio-scaled</p> </li> <li> <p>Blood Type (A, B, AB, O)</p> </li> <li> <p>Answer: Nominal (categories with no order)</p> </li> <li> <p>Likert Scale (Strongly Disagree to Strongly Agree)</p> </li> <li>Answer: Ordinal (ordered categories with unequal intervals)</li> </ol>"},{"location":"Chapter3.html#dataset-analysis-exercise","title":"Dataset Analysis Exercise","text":"<p>Given this product dataset, identify each attribute's type:</p> ProductID Category Price Size InStock CustomerRating Weight Color P1001 Electronics 599.99 Large Yes 4.5 2.3 Black P1002 Clothing 49.99 Medium No 3.8 0.5 Blue <p>Answers: - ProductID: Nominal - Category: Nominal - Price: Numeric, Continuous, Ratio-scaled - Size: Ordinal - InStock: Binary - CustomerRating: Numeric, Continuous, Interval-scaled - Weight: Numeric, Continuous, Ratio-scaled - Color: Nominal</p>"},{"location":"Chapter3.html#true-or-false","title":"True or False","text":"<ol> <li>Binary attributes are a special case of nominal attributes.</li> <li> <p>Answer: True</p> </li> <li> <p>For ordinal data, the difference between \"Good\" and \"Very Good\" is the same as between \"Fair\" and \"Good\".</p> </li> <li> <p>Answer: False</p> </li> <li> <p>Age can be treated as both discrete and continuous depending on context.</p> </li> <li> <p>Answer: True (discrete in years, continuous in precise measurement)</p> </li> <li> <p>The mean is a valid measure of central tendency for nominal data.</p> </li> <li>Answer: False</li> </ol>"},{"location":"Chapter3.html#calculation-practice","title":"Calculation Practice","text":"<p>Using the Student Records dataset, calculate:</p> <ol> <li>Valid statistics for Age attribute:</li> <li>Mean: (20+22+19+21+23+20+19+21)/8 = 20.625 years</li> <li>Median: 20.5 years</li> <li> <p>Mode: 20, 21 (bimodal)</p> </li> <li> <p>Valid statistics for GradeLevel:</p> </li> <li>Mode: Junior (appears 3 times)</li> <li> <p>Invalid: Mean grade level</p> </li> <li> <p>Valid statistics for Scholarship:</p> </li> <li>Frequency: Yes = 4, No = 4</li> <li>Mode: Bimodal (both categories equally frequent)</li> </ol>"},{"location":"Chapter3.html#critical-thinking-questions","title":"Critical Thinking Questions","text":"<ol> <li>Why can't we calculate the average of nominal data like \"Country\"?</li> <li> <p>Answer: Nominal values have no numerical significance or ordering, so mathematical operations like averaging are meaningless.</p> </li> <li> <p>When might you treat age as continuous instead of discrete?</p> </li> <li> <p>Answer: In medical studies or precise demographic analysis where exact age in years/months/days matters.</p> </li> <li> <p>What are the implications of misclassifying an ordinal attribute as nominal in machine learning?</p> </li> <li> <p>Answer: You lose valuable ordering information, potentially reducing model performance and interpretability.</p> </li> <li> <p>Why is temperature in Fahrenheit considered interval-scaled rather than ratio-scaled?</p> </li> <li>Answer: Because 0\u00b0F doesn't represent an absolute absence of heat, making ratios meaningless (100\u00b0F isn't twice as hot as 50\u00b0F).</li> </ol>"},{"location":"Chapter3.html#summary","title":"Summary","text":"<p>Understanding attribute types is fundamental to data analysis and machine learning. Each attribute type has specific properties that determine: - Which statistical operations are valid - How to properly encode the data - Which visualization techniques are appropriate - What machine learning algorithms can be applied</p> <p>Remember the key distinctions: - Nominal: Categories without order - Ordinal: Categories with meaningful order - Interval: Numeric with consistent intervals but no true zero - Ratio: Numeric with true zero and meaningful ratios</p>"},{"location":"Chapter4.html","title":"\ud83d\udcd8 Chapter 4 \u2014 Basic Statistical Descriptions of Data","text":"<p>Measuring central tendency (Mean, Median, Mode) and dispersion (Range, Quartiles, Variance, Standard Deviation, Interquartile Range). Includes worked examples, formulas.</p>"},{"location":"Chapter4.html#1-central-tendency","title":"1. Central Tendency","text":""},{"location":"Chapter4.html#11-mean-arithmetic-average","title":"1.1 Mean (Arithmetic Average)","text":"<p>For a sample \\(x_1,\\dots,x_n\\), </p> \\[ \\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n} x_i. \\]"},{"location":"Chapter4.html#12-median-50th-percentile","title":"1.2 Median (50<sup>th</sup> Percentile)","text":"<p>Middle value after sorting; for even \\(n\\), average the two middle values.</p>"},{"location":"Chapter4.html#13-mode","title":"1.3 Mode","text":"<p>Most frequent value (or, for continuous data, the modal bin with highest frequency).</p> <p>Figure: Histogram with mean/median/mode markers </p>"},{"location":"Chapter4.html#2-dispersion-of-data","title":"2. Dispersion of Data","text":""},{"location":"Chapter4.html#21-range","title":"2.1 Range","text":"\\[ \\text{Range} = \\max(x) - \\min(x). \\]"},{"location":"Chapter4.html#22-quartiles-iqr","title":"2.2 Quartiles &amp; IQR","text":"<p>Quartiles \\(Q_1, Q_2, Q_3\\) split the ordered data into four parts. Interquartile Range (IQR):</p> \\[ \\text{IQR} = Q_3 - Q_1. \\] <p>Figure: Boxplot with quartiles and whiskers </p>"},{"location":"Chapter4.html#23-variance-standard-deviation","title":"2.3 Variance &amp; Standard Deviation","text":"<p>Sample variance \\(s^2\\) and standard deviation \\(s\\):</p> \\[ s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar{x})^2, \\qquad s = \\sqrt{s^2}. \\] <p>Figure: Deviations from the mean (visualizing variance &amp; std) </p>"},{"location":"Chapter4.html#3-worked-numerical-example-by-hand","title":"3. Worked Numerical Example (by hand)","text":"<p>Given the sample (n=10): \\(\\{6,\\, 7,\\, 3,\\, 9,\\, 10,\\, 6,\\, 8,\\, 4,\\, 7,\\, 6\\}\\)</p> <ol> <li>Sort: \\( \\{3,4,6,6,6,7,7,8,9,10\\} \\) </li> <li>Mean: \\( \\bar{x}=\\frac{3+4+6+6+6+7+7+8+9+10}{10}=6.6 \\) </li> <li>Median: average of 5<sup>th</sup> &amp; 6<sup>th</sup> elements \\( \\Rightarrow (6+7)/2=6.5 \\) </li> <li>Mode: 6 (most frequent)  </li> <li>Range: \\(10-3=7\\) </li> <li>Quartiles (Tukey): </li> <li>\\(Q_1=\\) median of lower half \\(\\{3,4,6,6,6\\}=6\\) </li> <li>\\(Q_2=\\) median \\(=6.5\\) </li> <li>\\(Q_3=\\) median of upper half \\(\\{7,7,8,9,10\\}=8\\) </li> <li>\\(\\text{IQR}=Q_3-Q_1=2\\) </li> <li>Variance &amp; Std: compute \\(\\sum (x_i-\\bar{x})^2 = 46.4\\) </li> <li>\\(s^2 = 46.4/(10-1) \\approx 5.156\\) </li> <li>\\(s \\approx 2.271\\)</li> </ol>"},{"location":"Chapter4.html#4-practice-questions","title":"4. Practice Questions","text":"<ol> <li>Prove that the mean minimizes the sum of squared deviations \\( \\sum (x_i - c)^2 \\).  </li> <li>Show, by example, a dataset where mean \u2260 median due to skewness.  </li> <li>Compare IQR vs standard deviation as dispersion measures for heavy\u2011tailed data.  </li> <li>For a bimodal distribution, discuss the usefulness of \u201cmode\u201d and propose alternatives.</li> </ol>"},{"location":"Chapter5.html","title":"\ud83d\udcca Graphic Displays of Basic Statistical Descriptions of Data","text":""},{"location":"Chapter5.html#objective","title":"\ud83c\udfaf Objective","text":"<p>To understand and visualize basic statistical descriptions of data using graphical methods such as:</p> <ul> <li>Quantile Plot  </li> <li>Quantile-Quantile (Q-Q) Plot  </li> <li>Histogram  </li> <li>Quartile Plot (Box Plot)  </li> <li>Distribution Chart (Density Plot / KDE)</li> </ul>"},{"location":"Chapter5.html#quantile-plot","title":"Quantile Plot","text":""},{"location":"Chapter5.html#definition","title":"\u2733\ufe0f Definition","text":"<p>A quantile plot displays the ordered data against their cumulative probabilities. It helps visualize how data values are distributed and whether they deviate from a uniform or theoretical distribution.</p>"},{"location":"Chapter5.html#formula","title":"\ud83d\udcd0 Formula","text":"<p>For ordered data values  \\( x_{(1)} \\le x_{(2)} \\le ... \\le x_{(n)} \\) :  </p> \\[ p_i = \\frac{i - 0.5}{n} \\] <p>Each data point is plotted as \\(( (p_i, x_{(i)}) )\\).</p>"},{"location":"Chapter5.html#manual-example","title":"\ud83e\uddee Manual Example","text":"<p>Given data: [10, 12, 15, 18, 20]</p> i x(i) p = (i-0.5)/n 1 10 0.1 2 12 0.3 3 15 0.5 4 18 0.7 5 20 0.9 <p>Plot these \\( (p, x) \\) points to get the quantile plot.</p> <p></p>"},{"location":"Chapter5.html#quantilequantile-qq-plot","title":"Quantile\u2013Quantile (Q\u2013Q) Plot","text":""},{"location":"Chapter5.html#definition_1","title":"\u2733\ufe0f Definition","text":"<p>A Q\u2013Q plot compares the quantiles of the sample data with those of a theoretical distribution (usually normal).  </p>"},{"location":"Chapter5.html#manual-example_1","title":"\ud83e\uddee Manual Example","text":"<p>Given sample data: [5, 6, 7, 8, 9] (n=5)</p> <p>Step 1: Compute ordered z-values (theoretical quantiles for normal distribution).  </p> <p>For i = 1 to n:</p> <p>\\([ p_i = \\frac{i - 0.5}{n} ]\\)</p> i Data \\( x_i \\) \\( p_i \\) \\( z(p_i) \\) from Z-table 1 5 0.1 -1.28 2 6 0.3 -0.52 3 7 0.5 0.00 4 8 0.7 0.52 5 9 0.9 1.28 <p>Plot \\( (z, x) \\). If points form a straight line, data ~ Normal.</p> <p></p>"},{"location":"Chapter5.html#references","title":"\ud83e\udde9 References","text":"<ol> <li>Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.  </li> <li>Cleveland, W. S. (1993). Visualizing Data. Hobart Press.  </li> <li>Seaborn Documentation: https://seaborn.pydata.org  </li> <li>Matplotlib Documentation: https://matplotlib.org</li> </ol>"},{"location":"Chapter6.html","title":"Chapter 6 - Statistical Methods","text":""},{"location":"Chapter6.html#1-histogram","title":"1. Histogram","text":""},{"location":"Chapter6.html#definition","title":"\u2733\ufe0f Definition","text":"<p>A histogram shows frequency distribution of data grouped into bins.</p> <p>\ud83e\uddee Formula</p> <p>Frequency in bin \ud835\udc56</p> <p>\\( f_\ud835\udc56 \\) = Number\u00a0of\u00a0observations\u00a0in\u00a0bin\u00a0\ud835\udc56 </p>"},{"location":"Chapter6.html#manual-example","title":"\ud83e\uddee Manual Example","text":"<p>Data: [2, 3, 3, 4, 5, 6, 7, 8, 8, 9]</p> <p>Bin width = 2 Bins: (2\u20134), (4\u20136), (6\u20138), (8\u201310)</p> Bin Frequency 2\u20134 3 4\u20136 2 6\u20138 3 8\u201310 2 <p>Plot bars with heights as frequencies.</p> <p></p>"},{"location":"Chapter6.html#2-box-plot-quartile-visualization","title":"2. Box Plot (Quartile Visualization)","text":""},{"location":"Chapter6.html#definition_1","title":"\u2733\ufe0f Definition","text":"<p>A box plot summarizes data based on five-number summary:</p> \\[\\text{Minimum, Q1, Median, Q3, Maximum}\\]"},{"location":"Chapter6.html#manual-example_1","title":"\ud83e\uddee Manual Example","text":"\\[data: [10, 12, 14, 16, 18, 20, 22]\\] \\[n = 7\\] \\[Median = 16  \\] \\[Q1 = 12 + (14-12)*0.5 = 13  \\] \\[Q3 = 20 + (22-20)*0.5 = 21\\] \\[Five-number\\quad summary = (10, 13, 16, 21, 22)\\] <p>Draw box from Q1\u2013Q3, line at Median, whiskers to min &amp; max.</p> <p></p> <p>&lt;</p>"},{"location":"Chapter6.html#3-distribution-chart-kde-plot","title":"3. Distribution Chart (KDE Plot)","text":""},{"location":"Chapter6.html#definition_2","title":"\u2733\ufe0f Definition","text":"<p>A KDE plot estimates probability density by smoothing the histogram.</p>"},{"location":"Chapter6.html#i-what-is-kde","title":"i) What is KDE?","text":"<p>Given a univariate sample \\((x_1,\\dots,x_n)\\), the KDE at a point \\((x)\\) is</p> \\[\\hat f_h(x) \\;=\\; \\frac{1}{n h}\\sum_{i=1}^n K\\!\\left(\\frac{x-x_i}{h}\\right)\\] <p>where: - \\((K(\\cdot))\\) is a kernel (e.g., Gaussian, Epanechnikov, Uniform, Triangular), \\((\\int K(u)\\,du=1)\\),</p> <ul> <li>\\((h&gt;0)\\) is the bandwidth (smoothing parameter).</li> </ul> <p>Intuition. Each observation contributes a small \u201cbump\u201d centered at \\((x_i)\\); the final curve is their average. Smaller \\((h)\\) \u2192 more wiggly; larger \\((h)\\) \u2192 smoother.</p>"},{"location":"Chapter6.html#ii-common-kernels-closed-forms","title":"ii) Common Kernels (closed forms)","text":"<ul> <li> <p>Gaussian: \\((K(u)=\\dfrac{1}{\\sqrt{2\\pi}}e^{-u^2/2})\\)</p> </li> <li> <p>Epanechnikov: \\((K(u)=\\dfrac{3}{4}(1-u^2)\\mathbf{1}\\{|u|\\le 1\\})\\)</p> </li> <li> <p>Uniform (Rectangular): \\((K(u)=\\dfrac12\\mathbf{1}\\{|u|\\le 1\\})\\)</p> </li> <li> <p>Triangular: \\((K(u)=(1-|u|)\\mathbf{1}\\{|u|\\le 1\\})\\)</p> </li> </ul> <p>In practice, the bandwidth matters more than the specific kernel choice for smooth densities.</p>"},{"location":"Chapter6.html#iii-manual-example-a-gaussian-kernel-explicit-numbers","title":"iii) Manual Example A \u2014 Gaussian kernel, explicit numbers","text":"<p>Dataset: \\((x=\\{4,5,6,7,8\\})\\) (so \\((n=5)\\)).  </p> <p>Useful Gaussian values (to 6 d.p.): \\(\\phi(0)=0.398942,\\phi(0.5)=0.352065,\\phi(1)=0.241971\\)</p> <p>\\(\\phi(1.5)=0.129518,\\phi(2)=0.053991,\\phi(2.5)=0.017528\\)</p> <p>\\(\\phi(3)=0.004432,\\phi(4)=0.000134.\\)</p>"},{"location":"Chapter6.html#a1-kde-at-x6-with-bandwidth-h1","title":"A1) KDE at \\((x=6)\\) with bandwidth \\((h=1)\\)","text":"<p>Standardized offsets \\((u_i=(x- x_i)/h)\\): \\(((2,1,0,-1,-2))\\).  </p> <p>Contributions: \\((\\phi(2),\\phi(1),\\phi(0),\\phi(1),\\phi(2))\\).  </p> <p>\\(\\sum \\phi = 0.053991 + 0.241971 + 0.398942 + 0.241971 + 0.053991 = 0.990866.\\)</p> <p>\\(\\hat f_{h=1}(6) = \\frac{1}{n h}\\sum \\phi = \\frac{0.990866}{5\\cdot 1} = \\boxed{0.198173}.\\)</p>"},{"location":"Chapter6.html#a2-kde-at-x5-with-h1","title":"A2) KDE at \\((x=5)\\) with \\((h=1)\\)","text":"<p>Offsets \\((u=(1,0,-1,-2,-3))\\) \u2192 \\((\\phi(1)+\\phi(0)+\\phi(1)+\\phi(2)+\\phi(3))\\).</p> <p>\\(\\sum \\phi = 0.241971 + 0.398942 + 0.241971 + 0.053991 + 0.004432 = 0.941307.\\)</p> <p>\\(\\hat f_{h=1}(5) = \\frac{0.941307}{5} = \\boxed{0.188261}.\\)</p>"},{"location":"Chapter6.html#a3-effect-of-bandwidth-at-x6","title":"A3) Effect of bandwidth at \\((x=6)\\)","text":"<ul> <li>\\((h=0.5)\\): \\((u=(4,2,0,-2,-4))\\) \u2192 \\((\\phi(4)+\\phi(2)+\\phi(0)+\\phi(2)+\\phi(4))\\) </li> </ul> <p>\\((\\sum \\phi = 0.000134 + 0.053991 + 0.398942 + 0.053991 + 0.000134 = 0.507192)\\).  </p> <p>\\((\\hat f = \\dfrac{1}{n h}\\sum = \\dfrac{0.507192}{5\\cdot 0.5} = \\boxed{0.202877})\\).</p> <ul> <li>\\((h=2)\\): \\((u=(1,0.5,0,-0.5,-1))\\) \u2192 \\((\\phi(1)+\\phi(0.5)+\\phi(0)+\\phi(0.5)+\\phi(1))\\) </li> </ul> <p>\\((\\sum \\phi = 0.241971 + 0.352065 + 0.398942 + 0.352065 + 0.241971 = 1.587014)\\).  </p> <p>\\((\\hat f = \\dfrac{1.587014}{5\\cdot 2} = \\boxed{0.158701})\\).</p> <p>Observation. Increasing \\((h)\\) lowered the peak (more smoothing). Decreasing \\((h)\\) raised the peak/sharpness.</p>"},{"location":"Chapter6.html#iv-manual-example-b-epanechnikov-kernel-h1","title":"iv) Manual Example B \u2014 Epanechnikov kernel, \\((h=1)\\)","text":"<p>At \\((x=6)\\): \\((u=(2,1,0,-1,-2))\\). Only \\((|u|\\le 1)\\) contribute: \\((u=1,0,-1)\\). </p> <p>\\((K(1)=0,\\;K(0)=\\tfrac34,\\;K(-1)=0)\\).  </p> <p>\\(\\sum K = 0 + 0.75 + 0 = 0.75,\\quad \\hat f = \\frac{0.75}{n h} = \\frac{0.75}{5\\cdot 1} = \\boxed{0.15}.\\)</p>"},{"location":"Chapter6.html#v-choosing-bandwidth-by-hand-closedform-rules","title":"v) Choosing Bandwidth by Hand (closed\u2011form rules)","text":"<p>Let \\((s)\\) be sample standard deviation; \\((\\text{IQR}=Q_3-Q_1)\\). </p> <p>Define \\((n^{-1/5})\\) (for \\((n=5)\\), \\((n^{-1/5}\\approx 0.72478)\\)).</p> <ul> <li> <p>Scott\u2019s rule (Normal reference): \\(h_{\\text{Scott}} = 1.06\\, s\\, n^{-1/5}.\\)</p> </li> <li> <p>Silverman\u2019s rule of thumb: \\(h_{\\text{Silv}} = 0.9\\;\\min\\!\\left(s, \\frac{\\text{IQR}}{1.34}\\right)\\! n^{-1/5}.\\)</p> </li> </ul> <p>For our dataset \\( x=\\{4,5,6,7,8\\} \\): - Mean \\(=6 \\). Using sample SD: \\((s=\\sqrt{\\tfrac{\\sum(x_i-6)^2}{n-1}}=\\sqrt{\\tfrac{10}{4}}=1.58114)\\) </p> <ul> <li> <p>Quartiles (hinges): \\((Q_1\\approx 5,\\; Q_3\\approx 7 \\Rightarrow \\text{IQR}=2)\\), \\((\\text{IQR}/1.34=1.49254)\\). </p> </li> <li> <p>\\((n^{-1/5}\\approx 0.72478)\\).</p> </li> </ul> <p>Hence, \\(h_{\\text{Scott}} \\approx 1.06\\times 1.58114 \\times 0.72478 = \\boxed{1.215},\\)</p> <p>\\(h_{\\text{Silv}} \\approx 0.9\\times \\min(1.58114,\\,1.49254)\\times 0.72478 = 0.9\\times 1.49254 \\times 0.72478 \\approx \\boxed{0.973}.\\)</p>"},{"location":"Chapter6.html#vi-handcomputation-template-examready","title":"vi) Hand\u2011Computation Template (exam\u2011ready)","text":"<p>Input: sample \\((x_1,\\dots,x_n)\\), kernel \\((K)\\), bandwidth \\((h)\\), target evaluation point \\((x^\\star)\\). Steps: 1. Compute offsets \\((u_i=\\dfrac{x^\\star - x_i}{h})\\). </p> <ol> <li> <p>For each \\((u_i)\\), evaluate \\((K(u_i))\\) (using table/known values).  </p> </li> <li> <p>Sum: \\((S=\\sum_{i=1}^n K(u_i))\\).  </p> </li> <li> <p>Output: \\((\\hat f(x^\\star)=\\dfrac{S}{n h})\\).  </p> </li> <li> <p>Repeat for a grid of \\((x^\\star)\\) to sketch the curve by hand.</p> </li> </ol>"},{"location":"Chapter6.html#vii-worked-minigrid-gaussian-h1","title":"vii) Worked Mini\u2011Grid (Gaussian, \\((h=1)\\))","text":"<p>Dataset as before. Evaluate \\((\\hat f(x))\\) at \\((x=\\{5,6,7\\})\\). Because of symmetry, \\((\\hat f(5)=\\hat f(7))\\).</p> \\((x)\\) Offsets \\((u)\\) for \\((\\{4,5,6,7,8\\})\\) Sum of \\((\\phi(u))\\) \\((\\hat f(x)=\\dfrac{1}{5}\\sum \\phi(u))\\) 5 (1,0,-1,-2,-3) 0.941307 0.188261 6 (2,1,0,-1,-2) 0.990866 0.198173 7 (3,2,1,0,-1) 0.941307 0.188261"},{"location":"Chapter6.html#viii-comparing-kde-vs-histogram-manual","title":"viii) Comparing KDE vs Histogram (manual)","text":"<p>Same data, bins of width 2: (2\u20134], (4\u20136], (6\u20138], (8\u201310] Frequencies: 0, 2, 3, 0 \u2192 bars at 0, 2, 3, 0. KDE is smooth and does not depend on bin edges; histogram does.</p>"},{"location":"Chapter6.html#ix-boundary-bias-discrete-data","title":"ix) Boundary Bias &amp; Discrete Data","text":"<ul> <li>For supports like \\(([0,\\infty))\\), Gaussian kernels \u201cspill\u201d mass into negatives near 0. Fixes: boundary\u2011corrected kernels, reflection, or transformation (e.g., log).  </li> <li>KDE assumes continuous data; for discrete/integer data, consider jittering or discrete kernels.</li> </ul>"},{"location":"Chapter6.html#x-multimodality-by-hand-quick-check","title":"x) Multimodality by Hand (quick check)","text":"<p>If data cluster near two centers, KDE will show two peaks when \\((h)\\) is small enough to resolve them. Try two clusters (e.g., \\((\\{0,0.2,0.1\\}\\cup\\{3,3.1,3.2\\})\\)) and compute at midpoints to see valley between modes.</p>"},{"location":"Chapter6.html#xi-short-exam-problems-with-answers","title":"xi) Short Exam Problems (with answers)","text":"<p>Q1. For \\((x=\\{4,5,6,7,8\\})\\), Gaussian kernel, \\((h=1)\\), compute \\((\\hat f(6))\\). A. \\((0.1982)\\) (from \u00a73 A1).</p> <p>Q2. Same data, Epanechnikov, \\((h=1)\\), compute \\((\\hat f(6))\\). A. \\((0.15)\\) (from \u00a74).</p> <p>Q3. Using Silverman\u2019s rule, approximate \\((h)\\). A. \\((\\approx 0.973)\\) (from \u00a75).</p> <p>Q4. Explain effect of doubling \\((h)\\) on the KDE at a peak. A. Peak height typically decreases; curve becomes smoother (see \u00a73 A3).</p>"},{"location":"Chapter6.html#xii-quick-reference-values-gaussian-phiu","title":"xii) Quick Reference Values (Gaussian \\((\\phi(u))\\))","text":"<p>\\(\\small\\begin{array}{c|cccccccc} u &amp; 0 &amp; 0.5 &amp; 1 &amp; 1.5 &amp; 2 &amp; 2.5 &amp; 3 &amp; 4\\\\\\hline \\phi(u) &amp; 0.399 &amp; 0.352 &amp; 0.242 &amp; 0.130 &amp; 0.054 &amp; 0.018 &amp; 0.004 &amp; 0.0001 \\end{array}\\)</p> <p></p>"},{"location":"Chapter6.html#summary-table","title":"Summary Table","text":"Visualization Purpose Manual Computation Quantile Plot Plot cumulative probabilities (i-0.5)/n vs x(i) Q\u2013Q Plot Compare data quantiles to theoretical z-scores vs data Histogram Frequency count Bin width, frequencies Box Plot Quartile-based summary Min, Q1, Median, Q3, Max KDE Plot Smooth probability density Kernel formula"},{"location":"Chapter6.html#references","title":"\ud83e\udde9 References","text":"<ol> <li>Tukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.  </li> <li>Cleveland, W. S. (1993). Visualizing Data. Hobart Press.  </li> <li>Seaborn Documentation: https://seaborn.pydata.org  </li> <li>Matplotlib Documentation: https://matplotlib.org</li> </ol>"},{"location":"Chapter7.html","title":"\ud83d\udcca Multivariate Visualization \u2014 Scatter Plot, Multiple Scatter, Bubble Chart, Density Chart","text":"<p>Goal: Provide PhD-level theory, math, hand-computable numerical examples.</p> <ul> <li>Scatter plot (bivariate)</li> <li>Scatter multiple (grouped scatter + scatter matrix)</li> <li>Bubble chart</li> <li>Density chart (2D KDE, contour, hexbin)</li> </ul>"},{"location":"Chapter7.html#1-scatter-plot-bivariate","title":"1) Scatter Plot (Bivariate)","text":""},{"location":"Chapter7.html#11-theory","title":"1.1 Theory","text":"<p>A scatter plot shows paired observations \\(((x_i, y_i) )\\) as points in \\((\\mathbb{R}^2 )\\). It reveals association, trend, clusters, and outliers.</p> <p>Pearson correlation (strength of linear association): \\(r=\\frac{\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar x)^2}\\sqrt{\\sum_{i=1}^n (y_i-\\bar y)^2}}.\\)</p>"},{"location":"Chapter7.html#12-hand-numerical-example","title":"1.2 Hand Numerical Example","text":"<p>Data (n=5): \\((x=[1,2,3,4,5])\\), \\((y=[2,3,5,4,7])\\).  </p> <p>\\((\\bar x=3)\\), \\((\\bar y=4.2)\\).</p> <p>Compute sums: \\((\\sum(x_i-\\bar x)(y_i-\\bar y)\\) \\(\\qquad\\qquad\\qquad = (\u22122)(\u22122.2)+(\u22121)(\u22121.2)+(0)(0.8)+(1)(\u22120.2)+(2)(2.8)\\) \\(\\qquad \\qquad \\qquad = 4.4+1.2+0\u22120.2+5.6=11.0)\\) </p> <p>\\((\\sum(x_i-\\bar x)^2= 4+1+0+1+4=10)\\) \\((\\sum(y_i-\\bar y)^2= 4.84+1.44+0.64+0.04+7.84=14.8)\\).  </p> <p>\\((r=11/\\sqrt{10\\cdot 14.8}=11/\\sqrt{148}=11/12.165=0.904)\\) (~).</p> <p></p>"},{"location":"Chapter7.html#2-scatter-multiple","title":"2) Scatter Multiple","text":"<p>Two common meanings:</p> <p>1) Grouped scatter (category-wise coloring/markers) to compare clusters/groups. 2) Scatter matrix to view pairwise relationships across many variables.</p>"},{"location":"Chapter7.html#21-theory-grouped-scatter","title":"2.1 Theory (Grouped Scatter)","text":"<p>Given groups \\((g\\in\\{1,\\dots,G\\})\\), plot \\(((x_i, y_i))\\) with style/color by \\((g_i)\\).  This reveals between-group separation and within-group variance.</p>"},{"location":"Chapter7.html#22-hand-numerical-example-grouped","title":"2.2 Hand Numerical Example (Grouped)","text":"<p>Data with group labels:  </p> <ul> <li>Group A: \\(((1,1.5),(2,2.2),(3,2.8))\\) </li> <li>Group B: \\(((1,2.6),(2,3.2),(3,3.8))\\)</li> </ul> <p>Visual expectation: B sits above A (larger \\((y)\\) for same \\((x)\\)).</p> <p></p>"},{"location":"Chapter7.html#24-scatter-matrix-pairwise-scatter-for-p-variables","title":"2.4 Scatter Matrix (Pairwise Scatter for \\((p)\\) variables)","text":"<p>Idea: For a dataset \\((X \\in \\mathbb{R}^{n\\times p})\\), visualize all \\(\\binom{p}{2}\\) pairs to assess linear/nonlinear relationships.</p> <p>Correlation matrix: \\((R = [r_{jk}]_{p\\times p})\\), where \\((r_{jk})\\) is Pearson correlation between \\((X_{\\cdot j}\\) and \\((X_{\\cdot k})\\).</p> <p></p>"},{"location":"Chapter7.html#3-bubble-chart","title":"3) Bubble Chart","text":""},{"location":"Chapter7.html#31-theory","title":"3.1 Theory","text":"<p>A bubble chart extends scatter by mapping a third (and possibly fourth) variable to marker size (and color). For triplets \\((x_i,y_i,s_i)\\), bubble area is proportional to \\((s_i)\\) (not radius), i.e. marker size \\((\\propto s_i)\\).</p> <p>Common scaling: If \\((s_i)\\) are raw magnitudes, use size in points\\((^2)\\):  </p> \\[size_i = \\alpha \\cdot \\frac{s_i - \\min s}{\\max s - \\min s + \\epsilon} + \\beta\\] <p>to avoid vanishing/huge bubbles.</p>"},{"location":"Chapter7.html#32-hand-numerical-example","title":"3.2 Hand Numerical Example","text":"<p>Data: \\(((x,y,s))\\) \\(((1,3,10), (2,4,30), (3,5,50))\\). Let \\((\\alpha=1000)\\), \\((\\beta=50)\\), \\((\\epsilon=0)\\). \\((\\min s=10, \\max s=50)\\). Normalized sizes: - for \\((10)\\): \\((50)\\) (smallest) - for \\((30)\\): \\((50 + 1000\\cdot (20/40)=50+500=550)\\) - for \\((50)\\): \\((50 + 1000\\cdot (40/40)=1050)\\)</p> <p></p>"},{"location":"Chapter7.html#4-density-chart-2d-density-kde-contour-hexbin","title":"4) Density Chart (2D Density: KDE, Contour, Hexbin)","text":""},{"location":"Chapter7.html#41-theory-2d-kernel-density-estimation-gaussian","title":"4.1 Theory: 2D Kernel Density Estimation (Gaussian)","text":"<p>Given points \\((\\{ \\mathbf{x}_i \\}_{i=1}^n \\subset \\mathbb{R}^2)\\), \\((\\mathbf{x}=(x,y))\\), diagonal bandwidth \\((H=\\mathrm{diag}(h_x^2, h_y^2))\\). The 2D Gaussian KDE is$ </p> <p>\\(\\hat f(\\mathbf{x})=\\frac{1}{n\\,2\\pi h_x h_y}\\sum_{i=1}^n \\exp\\!\\left(-\\tfrac{1}{2}\\left[\\left(\\frac{x-x_{i}}{h_x}\\right)^2+\\left(\\frac{y-y_{i}}{h_y}\\right)^2\\right]\\right).\\)</p> <p>Contours of \\((\\hat f(\\mathbf{x}))\\) yield </p> <p>density/contour plots; discretization via counts yields hexbin.</p>"},{"location":"Chapter7.html#42-hand-numerical-example-evaluate-kde-at-one-location","title":"4.2 Hand Numerical Example (Evaluate KDE at one location)","text":"<p>Data \\((n=3)\\): \\(((0,0), (1,0), (0,1))\\). </p> <p>Let \\((h_x=h_y=1)\\). </p> <p>Evaluate at \\(((x,y)=(0,0))\\).  </p> <p>Offsets and exponent terms:</p> <ul> <li>From \\(((0,0))\\): exponent \\((= -\\tfrac{1}{2}(0^2+0^2)=0)\\) \u2192 \\((\\exp(0)=1)\\)</li> <li>From \\(((1,0))\\): exponent \\((= -\\tfrac{1}{2}((\u22121)^2+0^2)=-0.5)\\) \u2192 \\((e^{-0.5}=0.60653)\\)</li> <li>From \\(((0,1))\\): exponent \\((= -\\tfrac{1}{2}(0^2+(\u22121)^2)=-0.5)\\) \u2192 \\((0.60653)\\)</li> </ul> <p>Sum \\((=1+0.60653+0.60653=2.21306)\\).  </p> <p>\\(\\hat f(0,0)=\\frac{1}{n\\,2\\pi h_x h_y}\\sum \\exp(\\cdot)=\\frac{2.21306}{3\\cdot 2\\pi\\cdot 1\\cdot 1}=\\frac{2.21306}{18.8496}=\\mathbf{0.1175}\\;(\\text{approx}).\\)</p>"},{"location":"Chapter7.html#5-practical-guidance-exam-tips","title":"5) Practical Guidance (Exam Tips)","text":"<ul> <li>Scaling for bubble charts: map area (not radius) to the magnitude.  </li> <li>Overplotting: use transparency (<code>alpha</code>), smaller markers, or hexbin/density.  </li> <li>Correlation vs. Causation: high \\((r )\\) does not imply causality.  </li> <li>Bandwidth choice (2D KDE): start with \\((h_x,h_y )\\) near the sample SDs times \\((n^{-1/6} )\\) (2D analogue); adjust by visual diagnostics.  </li> <li>Scatter matrix: inspect both diagonal (univariate distributions) and off-diagonal (pairwise scatter).</li> </ul>"},{"location":"Chapter7.html#6-references","title":"6) References","text":"<ul> <li>Tukey (1977), Exploratory Data Analysis.  </li> <li>Cleveland (1993), Visualizing Data.  </li> <li>Scott (2015), Multivariate Density Estimation.  </li> <li>Matplotlib documentation (Pairwise scatter &amp; hexbin).</li> </ul>"},{"location":"Chapter8.html","title":"\ud83d\udcc8 Visualizing High-Dimensional Data","text":"<p>(Parallel Coordinates \u00b7 Deviation Chart \u00b7 Andrews Curve)</p>"},{"location":"Chapter8.html#1-introduction","title":"1\ufe0f\u20e3 Introduction","text":"<p>When data has more than three dimensions, standard 2D or 3D plots fail to reveal structure. We use specialized projection and transformation techniques to visualize multivariate patterns:</p> <ul> <li> <p>Parallel Coordinate Plots (PCP) \u2014 visualize multi-attribute records as lines.  </p> </li> <li> <p>Deviation Charts \u2014 highlight differences from a baseline (useful for comparisons).  </p> </li> <li> <p>Andrews Curves \u2014 map multivariate points to continuous periodic functions.</p> </li> </ul>"},{"location":"Chapter8.html#2-parallel-coordinate-chart-pcp","title":"2\ufe0f\u20e3 Parallel Coordinate Chart (PCP)","text":""},{"location":"Chapter8.html#theory","title":"Theory","text":"<p>Given \\(p\\) variables \\(( x_1, x_2, ..., x_p )\\), each record \\(( i )\\) is a vector \\((\\mathbf{x}^{(i)} = (x^{(i)}_1, x^{(i)}_2, \\dots, x^{(i)}_p))\\) Each variable is drawn on a parallel axis. </p> <p>Each record is plotted as a polyline connecting values across axes.</p>"},{"location":"Chapter8.html#mathematical-normalization","title":"Mathematical Normalization","text":"<p>\\((x'_{ij} = \\dfrac{x_{ij} - \\min(x_j)}{\\max(x_j) - \\min(x_j)})\\)</p>"},{"location":"Chapter8.html#manual-example","title":"Manual Example","text":"<p>Original data and normalized values (min=10, max=30 for all three variables):</p> Observation X1 X2 X3 A 10 20 30 B 20 10 40 C 30 30 20 <p>Normalized: \\((x'_{ij}=(x_{ij}-10)/(30-10))\\)</p> Observation X1' X2' X3' A 0.0 0.5 0.5 B 0.5 0.0 1.0 C 1.0 1.0 0.0"},{"location":"Chapter8.html#example-figure","title":"Example Figure","text":""},{"location":"Chapter8.html#3-deviation-chart","title":"3\ufe0f\u20e3 Deviation Chart","text":""},{"location":"Chapter8.html#theory_1","title":"Theory","text":"<p>A Deviation Chart displays differences from a baseline \\(( b_j )\\) for each variable \\(( j )\\): \\(( d_{ij} = x_{ij} - b_j )\\)</p>"},{"location":"Chapter8.html#manual-example_1","title":"Manual Example","text":"<p>Baseline \\((b=[50,30,20])\\). Observations: - A = [55, 25, 20] \u2192 \\((d_A=[+5,-5,0])\\) - B = [45, 35, 25] \u2192 \\((d_B=[-5,+5,+5])\\) - C = [60, 40, 15] \u2192 \\((d_C=[+10,+10,-5])\\)</p>"},{"location":"Chapter8.html#example-figure_1","title":"Example Figure","text":""},{"location":"Chapter8.html#4-andrews-curves","title":"4\ufe0f\u20e3 Andrews Curves","text":""},{"location":"Chapter8.html#theory_2","title":"Theory","text":"<p>Each data point in \\((\\mathbb{R}^p)\\) is represented by  </p> \\[ f_i(t) = \\frac{x_{i1}}{\\sqrt{2}} + x_{i2}\\sin(t) + x_{i3}\\cos(t) + x_{i4}\\sin(2t) + x_{i5}\\cos(2t) + \\cdots, \\] <p>with \\((t\\in[-\\pi,\\pi])\\). Curve similarity reflects data similarity.</p>"},{"location":"Chapter8.html#manual-example-3-features","title":"Manual Example (3 features)","text":"<p>For A(1,2,3) and B(2,1,0): $$ f_A(t) = 1/\\sqrt{2} + 2\\sin t + 3\\cos t,\\; f_B(t) = 2/\\sqrt{2} + \\sin t. $$ Evaluate at \\((t=0,\\pi/2,\\pi)\\) to compare values and shapes.</p>"},{"location":"Chapter8.html#example-figure_2","title":"Example Figure","text":""},{"location":"Chapter8.html#5-practice-problems-by-hand","title":"5\ufe0f\u20e3 Practice Problems (By Hand)","text":"<ol> <li>PCP: Normalize and sketch lines for A(1,2,3,4), B(4,3,2,1), C(2,2,2,2). Identify negatively correlated pairs.  </li> <li>Deviation: With baseline [10,20,30], compute deviations for P(12,18,33), Q(9,21,27) and interpret.  </li> <li>Andrews: Compute \\((f(t))\\) for x=(1,0,2) at \\((t=0,\\pi/2,\\pi)\\) and compare with y=(2,1,1).</li> </ol>"},{"location":"Chapter9.html","title":"\ud83d\udcca Chapter 9 \u2014 Data Science Process","text":"<p>A comprehensive overview of the end\u2011to\u2011end Data Science Process, from prior knowledge to deployment.</p>"},{"location":"Chapter9.html#1-overview","title":"\ud83d\udd01 1. Overview","text":"<p>The Data Science Process transforms raw data into actionable insights through systematic stages:</p> <pre><code>graph TD\n    A[Prior Knowledge] --&gt; B[Data Preparation]\n    B --&gt; C[Modeling]\n    C --&gt; D[Evaluation]\n    D --&gt; E[Deployment]</code></pre> <p>Each stage iteratively refines understanding and improves decision quality.</p>"},{"location":"Chapter9.html#2-prior-knowledge","title":"\ud83e\udde0 2. Prior Knowledge","text":""},{"location":"Chapter9.html#definition","title":"Definition","text":"<ul> <li>Domain expertise, business context, and problem framing.</li> <li>Hypotheses and expectations derived from experience or literature.</li> </ul>"},{"location":"Chapter9.html#tasks","title":"Tasks","text":"<ul> <li>Identify the objective (classification, regression, clustering, etc.).</li> <li>Define target variable(s).</li> <li>Select metrics relevant to success.</li> </ul>"},{"location":"Chapter9.html#example","title":"Example","text":"<p>Suppose you\u2019re building a loan default prediction model. Prior knowledge includes: - Customer demographics, credit history, and income behavior. - Bank risk policies. - Economic indicators.</p> <pre><code>graph LR\n    A[Domain Knowledge] --&gt; B[Feature Selection]\n    A --&gt; C[Metric Design]\n    A --&gt; D[Assumption Setting]</code></pre>"},{"location":"Chapter9.html#3-data-preparation","title":"\ud83e\uddf9 3. Data Preparation","text":""},{"location":"Chapter9.html#definition_1","title":"Definition","text":"<p>Converting raw, messy data into a structured and analyzable form.</p>"},{"location":"Chapter9.html#substeps","title":"Sub\u2011steps","text":"<ol> <li>Data Collection: from databases, APIs, logs, or files.</li> <li>Data Cleaning: handle missing values, outliers, and duplicates.</li> <li>Feature Engineering: derive new features, normalize, encode categorical variables.</li> <li>Splitting: train, validation, and test sets.</li> </ol> <pre><code>graph TD\n    A[Raw Data] --&gt; B[Cleaning]\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Splitting]\n    D --&gt; E[Modeling Ready Dataset]</code></pre>"},{"location":"Chapter9.html#example-by-hand","title":"Example (by hand)","text":"Step Action Example Missing Replace null income <code>income = mean(income)</code> Outliers Cap extreme age values <code>age = min(age, 80)</code> Encoding Convert gender <code>male\u21921, female\u21920</code>"},{"location":"Chapter9.html#4-modeling","title":"\ud83e\uddee 4. Modeling","text":""},{"location":"Chapter9.html#definition_2","title":"Definition","text":"<p>Selecting and training algorithms to represent data patterns.</p>"},{"location":"Chapter9.html#types-of-models","title":"Types of Models","text":"Problem Model Family Example Classification Logistic Regression, Decision Tree, SVM Predict customer churn Regression Linear, Ridge, Lasso, Random Forest Predict house price Clustering K\u2011Means, DBSCAN Group customers Dimensionality Reduction PCA, t\u2011SNE Visualization <pre><code>graph LR\n    A[Features] --&gt; B[Train Model]\n    B --&gt; C[Evaluate on Validation Set]\n    C --&gt; D[Parameter Tuning]</code></pre>"},{"location":"Chapter9.html#mathematical-foundation-example","title":"Mathematical Foundation (example)","text":"<p>For linear regression:</p> \\[ \\hat{y} = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j \\] <ul> <li>where \\(\\boldsymbol{\\beta} = (X^TX)^{-1}X^Ty\\)</li> </ul>"},{"location":"Chapter9.html#5-evaluation","title":"\ud83d\udcc8 5. Evaluation","text":""},{"location":"Chapter9.html#purpose","title":"Purpose","text":"<p>Assess model performance and generalization.</p>"},{"location":"Chapter9.html#metrics","title":"Metrics","text":"Type Metrics Classification Accuracy, Precision, Recall, F1, ROC\u2011AUC Regression RMSE, MAE, \\(R^2\\) Clustering Silhouette Score, Davies\u2011Bouldin Index"},{"location":"Chapter9.html#example-calculation","title":"Example Calculation","text":"<p>For predictions: \\([0.9, 0.3, 0.8, 0.1]\\), actual: \\([1,0,1,0]\\) Threshold = 0.5 \u2192 Predictions = [1,0,1,0] \u2192 Accuracy = 100%.</p> <pre><code>graph TD\n    A[Trained Model] --&gt; B[Evaluation Metrics]\n    B --&gt; C[Compare Models]\n    C --&gt; D[Select Best Model]</code></pre>"},{"location":"Chapter9.html#6-deployment","title":"\ud83d\ude80 6. Deployment","text":""},{"location":"Chapter9.html#definition_3","title":"Definition","text":"<p>Integrating the trained model into a production environment.</p>"},{"location":"Chapter9.html#methods","title":"Methods","text":"<ul> <li>Batch inference: periodic predictions (e.g., nightly).</li> <li>Online inference: real\u2011time API endpoint.</li> <li>Edge deployment: mobile or IoT model execution.</li> </ul> <pre><code>graph TD\n    A[Best Model] --&gt; B[API Service]\n    B --&gt; C[Application Integration]\n    C --&gt; D[User Feedback Loop]</code></pre>"},{"location":"Chapter9.html#example-architecture","title":"Example Architecture","text":"<pre><code>graph LR\n    A[Model File #40; .pkl/.onnx #41;] --&gt; B[Flask/FastAPI Service]\n    B --&gt; C[Docker Container]\n    C --&gt; D[Cloud Deployment &lt;br&gt; #40; AWS, Azure, GCP #41;]\n    D --&gt; E[Monitoring + Retraining]</code></pre>"},{"location":"Chapter9.html#7-iteration-and-continuous-learning","title":"\ud83d\udd04 7. Iteration and Continuous Learning","text":"<p>The process is cyclic \u2014 feedback from deployment improves earlier stages.</p> <pre><code>graph TD\n    E[Deployment Feedback] --&gt; D[Evaluation]\n    D --&gt; C[Modeling]\n    C --&gt; B[Data Preparation]\n    B --&gt; A[Prior Knowledge]</code></pre>"},{"location":"Chapter9.html#8-summary-table","title":"\ud83e\udde9 8. Summary Table","text":"Stage Key Actions Tools Prior Knowledge Define objective, assumptions Domain expertise, documentation Data Preparation Cleaning, transformation Pandas, SQL, Excel Modeling Algorithm training scikit\u2011learn, TensorFlow, PyTorch Evaluation Performance metrics sklearn.metrics, visualization Deployment Serve models Flask, FastAPI, Docker, AWS Sagemaker"},{"location":"Chapter9.html#9-quick-exam-questions","title":"\ud83e\udde0 9. Quick Exam Questions","text":"<ol> <li>Explain how domain knowledge impacts feature engineering. </li> <li>What are the differences between batch and online deployment? </li> <li>Which metric is suitable for imbalanced classification? </li> <li>Describe a feedback loop in deployed systems. </li> <li>Write the formula for a linear regression model. </li> </ol>"},{"location":"about.html","title":"About This Textbook","text":""},{"location":"about.html#about-this-textbook_1","title":"\ud83d\udcd6 About This Textbook","text":"<p>Welcome to the Introduction to Data Science \u2014 Textbook project.</p> <p>This open educational resource is designed to provide students, educators, and self-learners with a comprehensive yet practical introduction to Data Science, integrating mathematics, programming, visualization, and real-world problem solving.</p>"},{"location":"about.html#purpose-and-goals","title":"\ud83c\udfaf Purpose and Goals","text":"<p>Data Science lies at the intersection of mathematics, computer science, and domain expertise. This textbook aims to:</p> <ul> <li>Build conceptual clarity in core Data Science principles.  </li> <li>Provide hands-on examples in Python that can be run and modified locally.  </li> <li>Serve as a teaching companion for undergraduate or postgraduate Data Science courses.  </li> <li>Promote open access and reproducibility in data-driven research and learning.</li> </ul> <p>The content is organized to align with academic curricula while remaining accessible for independent learners.</p>"},{"location":"about.html#structure-of-the-book","title":"\ud83e\udde9 Structure of the Book","text":"<p>The material is divided into thematic parts:</p> Part Focus Example Chapters Part I Foundations of Data Science Introduction, What is Data Science, Data Types Part II Descriptive Statistics and Visualization Measures of Central Tendency, Dispersion, Graphical Methods Part III Data Preparation and Processing Data Cleaning, Integration, Reduction, Transformation Part IV Modeling and Evaluation Regression, Classification, Clustering, Model Metrics Part V Deployment and Applications Model Evaluation, Generalization, Case Studies <p>Each chapter includes: - \ud83d\udcd8 Theoretical background - \ud83e\uddee Mathematical formulation - \ud83e\udde0 Manual (\u201cby hand\u201d) examples - \ud83d\udcbb Python code to generate plots and figures</p>"},{"location":"about.html#intended-audience","title":"\ud83e\uddd1\u200d\ud83c\udfeb Intended Audience","text":"<p>This material is suitable for:</p> <ul> <li>Undergraduate and graduate students beginning in Data Science or AI  </li> <li>Faculty members preparing lecture content or lab exercises  </li> <li>Professionals and researchers refreshing mathematical and statistical concepts  </li> <li>Self-learners following an open-source data science curriculum</li> </ul> <p>No prior exposure to advanced machine learning is required, though a basic familiarity with Python, statistics, and data analysis concepts is beneficial.</p>"},{"location":"about.html#technical-stack","title":"\u2699\ufe0f Technical Stack","text":"Component Purpose MkDocs Material Website framework and navigation MathJax LaTeX-style rendering of formulas Mermaid Flowcharts and concept diagrams Matplotlib + NumPy Figure generation from Python scripts GitHub Pages Continuous deployment and versioning <p>All figures and diagrams in this textbook are reproducible using the included Python scripts, which are stored alongside each Markdown file.</p>"},{"location":"about.html#accessibility-and-open-use","title":"\ud83c\udf10 Accessibility and Open Use","text":"<p>This textbook is distributed under the Creative Commons BY\u2013NC 4.0 License. You are free to use, share, and adapt the material for non-commercial educational purposes, provided proper attribution is given.</p> <p>\u201cKnowledge grows when shared. The intent of this project is to make Data Science learning universally accessible.\u201d</p>"},{"location":"about.html#about-the-author","title":"\ud83d\udc68\u200d\ud83d\udcbb About the Author","text":"<p>Dr. J. M. Reddy Educator, Researcher, and Developer in Artificial Intelligence &amp; Data Science - Focus Areas: AI-guided Big Data Analytics, Machine Learning, and Educational Technologies - Projects include: automated debugging tools, AI-guided medical imaging, and outcome-based education systems  </p> <p>For collaborations or citations, please refer to the repository: \ud83d\udc49 https://github.com/jmreddy2106/Introduction-to-Data-Science-textbook</p>"},{"location":"about.html#citation","title":"\ud83d\udd17 Citation","text":"<p>If you use or reference this textbook in research, please cite it as:</p> <p>Reddy, J. M. (2025). Introduction to Data Science \u2014 Textbook (Version 1.0). GitHub Repository: https://github.com/jmreddy2106/Introduction-to-Data-Science-textbook</p>"},{"location":"about.html#acknowledgments","title":"\ud83e\udded Acknowledgments","text":"<p>This work draws inspiration from: - University-level Data Science syllabi and open courseware (MIT, Stanford, IITs) - The open-source scientific Python community - Contributions from educators, students, and reviewers supporting open education</p> <p>Special thanks to contributors who tested chapters, generated plots, and refined the visualizations.</p>"},{"location":"about.html#feedback","title":"\ud83d\udcac Feedback","text":"<p>Contributions, corrections, and suggestions are welcome! Open an issue or submit a pull request on GitHub.  </p> <p>\ud83d\udce7 Contact: issues/new on GitHub</p> <p>Last updated: 2025-10-31</p>"},{"location":"datasets.html","title":"Datasets","text":""},{"location":"datasets.html#chapter-datasets","title":"Chapter Datasets","text":""},{"location":"datasets.html#chapter-1","title":"Chapter 1","text":"<ul> <li>Dataset description and link</li> </ul>"},{"location":"datasets.html#chapter-2","title":"Chapter 2","text":"<ul> <li>Dataset description and link</li> </ul>"},{"location":"datasets.html#external-resources","title":"External Resources","text":"<ul> <li>Kaggle Datasets</li> <li>UCI Machine Learning Repository</li> </ul>"},{"location":"getting-started.html","title":"\ud83d\ude80 Getting Started","text":"<p>Welcome to the Introduction to Data Science \u2014 Textbook project! This guide will help you set up your environment and start exploring the chapters interactively.</p>"},{"location":"getting-started.html#prerequisites","title":"\ud83e\udde9 Prerequisites","text":"<p>Before you begin, ensure that you have:</p> <ul> <li>\ud83e\uddee Basic mathematics knowledge (algebra, probability, and statistics)  </li> <li>\ud83d\udcbb A computer with Python 3.9+ installed  </li> <li>\ud83e\udde0 A curiosity about data, algorithms, and discovery! </li> </ul> <p>Optional tools you may find helpful: - Jupyter Notebook / JupyterLab for running Python snippets - VS Code or PyCharm for editing Markdown and scripts - Git for version control and contributing improvements</p>"},{"location":"glossary.html","title":"Glossary","text":""},{"location":"glossary.html#a","title":"A","text":"<p>Algorithm: A step-by-step procedure for solving a problem.</p>"},{"location":"glossary.html#b","title":"B","text":"<p>Big Data: Datasets that are too large or complex for traditional data processing.</p>"},{"location":"glossary.html#c","title":"C","text":"<p>Coming soon...</p>"},{"location":"references.html","title":"References","text":""},{"location":"references.html#books","title":"Books","text":"<ol> <li>Reference 1</li> <li>Reference 2</li> </ol>"},{"location":"references.html#papers","title":"Papers","text":"<p>Coming soon...</p>"},{"location":"references.html#online-resources","title":"Online Resources","text":"<ul> <li>Python Documentation</li> <li>NumPy Documentation</li> </ul>"},{"location":"appendix/python.html","title":"Python Basics","text":""},{"location":"appendix/python.html#installation","title":"Installation","text":"<p>Instructions for installing Python...</p>"},{"location":"appendix/python.html#basic-syntax","title":"Basic Syntax","text":"<p>Coming soon...</p>"},{"location":"appendix/r.html","title":"R Basics","text":""},{"location":"appendix/r.html#installation","title":"Installation","text":"<p>Instructions for installing R...</p>"},{"location":"appendix/r.html#basic-syntax","title":"Basic Syntax","text":"<p>Coming soon...</p>"},{"location":"appendix/sql.html","title":"SQL Reference","text":""},{"location":"appendix/sql.html#basic-queries","title":"Basic Queries","text":""},{"location":"appendix/sql.html#select-statement","title":"SELECT Statement","text":"<p>Coming soon...</p>"}]}